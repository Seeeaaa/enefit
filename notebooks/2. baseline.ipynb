{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af09e75c-0bf4-4c48-a601-01db2a2f3bd6",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294f506-7ec6-42cb-88a3-07cf9ae2c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().resolve().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d9fc89-7313-4a85-8519-e9d590e69243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal Scrolling\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b609dd18-6839-426a-bc21-f94adbdf60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame, Series, Timedelta\n",
    "from scipy.ndimage import binary_dilation\n",
    "from sklearn.metrics import mean_absolute_error as MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb66e024-d3af-43ad-8a53-aa261e6c108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa8367-1fd6-49f1-87e7-a0abaf056807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loading import load_all_raw_data\n",
    "from utils.preprocessing import process_all_dfs\n",
    "from utils.merging import merge_all_dfs\n",
    "from utils.feature_engineering import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c517a23d-20f1-4d78-9500-cdad996b9962",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\n",
    "    \"display.float_format\",\n",
    "    lambda x: f\"{x:.2e}\" if abs(x) < 0.01 and x != 0 else f\"{x:.2f}\",\n",
    ")\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf64f8-3024-49c1-8003-2bf3bdf7a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = \"../data/raw_data/\"\n",
    "ADDITIONAL_DATA_PATH = \"../data/additional_data/\"\n",
    "\n",
    "SEGMENT_C = [\"county\", \"product_type\", \"is_business\"]\n",
    "CATEGORICAL_C = [\"county\", \"product_type\", \"is_business\", \"is_consumption\"]\n",
    "TARGET_C = [\n",
    "    \"county\",\n",
    "    \"product_type\",\n",
    "    \"is_business\",\n",
    "    \"is_consumption\",\n",
    "    \"datetime\",\n",
    "]\n",
    "RAND = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38031e-6fd2-4ad1-8c50-dbd1858776f6",
   "metadata": {},
   "source": [
    "# 2. Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744670d-0bd3-44cb-a39a-f71406dfcc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dfs = process_all_dfs(\n",
    "    load_all_raw_data(RAW_DATA_PATH, ADDITIONAL_DATA_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d97801-1f74-4776-b2e7-ce9234d10c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f9a1a-1d63-45d7-9f51-e7029aad6ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merge_all_dfs(processed_dfs, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b88997f-e282-4183-8432-b26b2bdfe988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_dst_flag(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b94a016-f449-4b8e-8e09-9c3abbe5fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_cyclic_datetime_features(df, drop_raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11122383-2f03-4716-a628-104dd4f8469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in [2, 3, 7]:\n",
    "# for lag in range(2, 15):\n",
    "    df = df.merge(\n",
    "        get_lag(processed_dfs[\"train\"][TARGET_C + [\"target\"]], lag=lag),\n",
    "        how=\"left\",\n",
    "        on=TARGET_C,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5bae2-9407-4fb6-a9fa-1414023b3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in [24, 24 * 3, 24 * 7, 24 * 14]:\n",
    "    # for window in [24 * i for i in range(1, 15)]:\n",
    "    df = df.merge(\n",
    "        get_moving_average(\n",
    "            processed_dfs[\"train\"]\n",
    "            .set_index(\"datetime\")\n",
    "            .sort_index()\n",
    "            .groupby(CATEGORICAL_C, observed=True, as_index=False),\n",
    "            columns=[\"target\"],\n",
    "            window=window,\n",
    "            # ).dropna(),\n",
    "        ),\n",
    "        how=\"left\",\n",
    "        on=TARGET_C,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc127e-7d7f-4455-a3de-eb55ffdb8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"t_over_cap\"] = (df[\"2d_lag_target\"] / df[\"installed_capacity\"]).astype(\n",
    "    \"float32\"\n",
    ")\n",
    "df[\"t_over_eic\"] = (df[\"2d_lag_target\"] / df[\"eic_count\"]).astype(\"float32\")\n",
    "df[\"cap_per_eic\"] = (df[\"installed_capacity\"] / df[\"eic_count\"]).astype(\n",
    "    \"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f19fac-bbc0-49cc-b0b9-cfcc89e02abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w, h = 20, 14\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(w, h))\n",
    "# # plt.figure(figsize=(w, h))\n",
    "\n",
    "# sns.heatmap(\n",
    "#     df.drop(\n",
    "#         columns=df.select_dtypes([\"category\"]).columns.tolist()\n",
    "#         + [\n",
    "#             \"datetime\",\n",
    "#             \"data_block_id\",\n",
    "#             \"date\",\n",
    "#         ]\n",
    "#     ).corr(),\n",
    "#     annot=True,\n",
    "#     fmt=\".1f\",\n",
    "#     annot_kws={\"size\": 7},\n",
    "# )\n",
    "\n",
    "\n",
    "# buf = io.BytesIO()\n",
    "# fig.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n",
    "# buf.seek(0)\n",
    "# img_base64 = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# html_code = (\n",
    "#     '<div style=\"overflow-x: auto; width: 100%;\">'\n",
    "#     '<img src=\"data:image/png;base64,{}\" style=\"display: block; max-width: none; width: auto;\">'\n",
    "#     \"</div>\"\n",
    "# ).format(img_base64)\n",
    "\n",
    "# display(HTML(html_code))\n",
    "# plt.close(fig)\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d86b9f-3922-4018-9a64-e7b06cb56c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_for_missing = df[TARGET_C + [\"target\"]].dropna().copy()\n",
    "\n",
    "# # Add hour index starting from the beginning\n",
    "# df_for_missing[\"hour_index\"] = (\n",
    "#     (df_for_missing[\"datetime\"] - df_for_missing[\"datetime\"].min())\n",
    "#     // pd.Timedelta(hours=1)\n",
    "# ).astype(np.uint16)\n",
    "\n",
    "\n",
    "# # Add a group index corresponding to the unique combination of all\n",
    "# # categorical features (i.e. this feature is not equal to\n",
    "# # prediction_unit_id): county, is_business, product_type,\n",
    "# # is_consumption.\n",
    "# # The maximum number of combinations is 16 * 4 * 2 * 2 = 256, but the\n",
    "# # actual number of observed values is less than 256\n",
    "# df_for_missing[\"group_index\"] = (\n",
    "#     df_for_missing.groupby(\n",
    "#         CATEGORICAL_C,\n",
    "#         observed=True,\n",
    "#     )\n",
    "#     .ngroup()\n",
    "#     .astype(np.uint8)\n",
    "# )\n",
    "\n",
    "# # Create a 2D array with a shape equal to the number of groups and the\n",
    "# # total number of hours between the minimum and maximum timestamps\n",
    "# n_groups = df_for_missing[\"group_index\"].nunique()\n",
    "# n_hours = df_for_missing[\"hour_index\"].max() + 1\n",
    "# missmap = np.full((n_groups, n_hours), np.nan)\n",
    "\n",
    "# # Fill the array with the corresponding flag values\n",
    "# missmap[df_for_missing[\"group_index\"], df_for_missing[\"hour_index\"]] = (\n",
    "#     df_for_missing[\"target\"] != 0\n",
    "# )\n",
    "\n",
    "# # Dilate missing values because of high density of plot and isolated\n",
    "# # missing values are not visible.\n",
    "# n = 6  # How many hours before and after count as missing\n",
    "# structure = np.ones((1, 2 * n + 1), dtype=bool)\n",
    "# missing_dilated = binary_dilation(np.isnan(missmap), structure=structure)\n",
    "# missmap[missing_dilated] = np.nan\n",
    "\n",
    "# alpha = 0.8\n",
    "\n",
    "# min_date = df_for_missing[\"datetime\"].min()\n",
    "# max_date = df_for_missing[\"datetime\"].max()\n",
    "# n_months = len(pd.date_range(start=min_date, end=max_date, freq=\"ME\"))\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(14, 7))\n",
    "# sns.heatmap(\n",
    "#     missmap,\n",
    "#     cmap=sns.color_palette([\"tab:blue\", \"tab:orange\"]),\n",
    "#     cbar=False,\n",
    "#     alpha=alpha,\n",
    "#     ax=ax,\n",
    "# )\n",
    "\n",
    "# xtick_indices = np.linspace(0, n_hours, num=n_months, dtype=int)\n",
    "# xtick_labels = pd.date_range(min_date, max_date, freq=\"ME\")\n",
    "\n",
    "\n",
    "# ax.set_xticks(xtick_indices)\n",
    "# ax.set_xticklabels(xtick_labels.strftime(\"%Y-%m\"), rotation=45, ha=\"right\")\n",
    "\n",
    "# dx, dy = 0, -2.5\n",
    "# for label in ax.get_yticklabels():\n",
    "#     offset = mt.ScaledTranslation(dx / 72, dy / 72, fig.dpi_scale_trans)\n",
    "#     label.set_transform(label.get_transform() + offset)\n",
    "\n",
    "# legend_patches = [\n",
    "#     mpatches.Patch(\n",
    "#         facecolor=\"white\",\n",
    "#         label=\"Missing value\",\n",
    "#         edgecolor=\"black\",\n",
    "#         linewidth=0.5,\n",
    "#     ),\n",
    "#     mpatches.Patch(\n",
    "#         facecolor=\"tab:blue\",\n",
    "#         alpha=alpha,\n",
    "#         label=\"Zero value\",\n",
    "#         edgecolor=\"black\",\n",
    "#         linewidth=0.5,\n",
    "#     ),\n",
    "#     mpatches.Patch(\n",
    "#         facecolor=\"tab:orange\",\n",
    "#         alpha=alpha,\n",
    "#         label=\"Not zero value\",\n",
    "#         edgecolor=\"black\",\n",
    "#         linewidth=0.5,\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "# plt.legend(\n",
    "#     handles=legend_patches,\n",
    "#     title=\"Data Presence\",\n",
    "#     title_fontsize=12,\n",
    "#     bbox_to_anchor=(1, 1),\n",
    "#     loc=\"upper left\",\n",
    "#     fontsize=11,\n",
    "#     frameon=False,\n",
    "# )\n",
    "\n",
    "# plt.title(\n",
    "#     \"Heatmap of time series gaps for all combinations \"\n",
    "#     \"of categorical features\",\n",
    "#     fontsize=13,\n",
    "# )\n",
    "# plt.xlabel(\"Month\", fontsize=10)\n",
    "# plt.ylabel(\"Group index\", fontsize=10)\n",
    "# plt.grid(False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f041bf-4858-46a3-9889-a1fb7b7cf085",
   "metadata": {},
   "source": [
    "# 3. Simple Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846595d0-3622-49ad-b87d-28a428d90760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_equal_days(\n",
    "    dt: Series,\n",
    "    train_days: int = 1,\n",
    "    fh_days: int = 1,  # Only daily predictions as per problem statement\n",
    "    val_splits: int = 1,\n",
    "    expanding: bool = True,\n",
    ") -> list[dict[str, tuple[pd.Timestamp, pd.Timestamp]]]:\n",
    "    dt = dt.dt.floor(\"D\")  # Processing on a day scope\n",
    "\n",
    "    train_days = Timedelta(days=train_days)\n",
    "    train_days_delta = train_days - Timedelta(days=1)  # Indexing from 0\n",
    "\n",
    "    fh_days = Timedelta(days=fh_days)\n",
    "    fh_days_delta = fh_days - Timedelta(days=1)  # Indexing from 0\n",
    "\n",
    "    first_day = dt.min()\n",
    "    last_day = dt.max()\n",
    "\n",
    "    # test_start = last_day - fh_days_delta\n",
    "    # test_end = last_day + pd.Timedelta(hours=23)\n",
    "\n",
    "    splits = []\n",
    "\n",
    "    intermediate_period_start = first_day + train_days\n",
    "    intermediate_period_end = last_day - fh_days\n",
    "    intermediate_period_days = (\n",
    "        intermediate_period_end - intermediate_period_start\n",
    "    ).days\n",
    "    base_step = Timedelta(days=intermediate_period_days // val_splits)\n",
    "    step_rem = Timedelta(days=intermediate_period_days % val_splits)\n",
    "\n",
    "    for step in range(val_splits):\n",
    "        offset = base_step * step + min(Timedelta(days=step), step_rem)\n",
    "        if expanding:\n",
    "            train_start = first_day\n",
    "            train_end = (\n",
    "                train_start\n",
    "                + train_days_delta\n",
    "                + offset\n",
    "                + pd.Timedelta(hours=23)\n",
    "            )\n",
    "        else:\n",
    "            train_start = first_day + offset\n",
    "            train_end = train_start + train_days_delta + pd.Timedelta(hours=23)\n",
    "\n",
    "        val_start = train_end + pd.Timedelta(hours=1)\n",
    "        val_end = val_start + fh_days_delta + pd.Timedelta(hours=23)\n",
    "        splits.append(\n",
    "            {\n",
    "                \"train\": (train_start, train_end),\n",
    "                \"val\": (val_start, val_end),\n",
    "                # \"test\": (test_start, test_end),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ed027-3ff0-41c3-b977-e4db60bebfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First timestamp: 2021-09-01 00:00:00\n",
      "Last timestamp: 2023-05-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"First timestamp: {df[\"datetime\"].min()}\",\n",
    "    f\"Last timestamp: {df[\"datetime\"].max()}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e7c10-53c4-46a7-81e6-3ca00042b929",
   "metadata": {},
   "source": [
    "Since the minimum usable lag for prediction is 48 hours, this horizon is particularly well suited as a naive baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47113607-a276-453d-8fa8-426e8863e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_TO_DROP = [\"datetime\", \"data_block_id\", \"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a5f67b-7aed-4dc3-b34b-29db0c3c7b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last day\n",
    "test_end = df[\"datetime\"].max()\n",
    "test_start = test_end.normalize()  # 00 - 23\n",
    "df_test = df.loc[df[\"datetime\"] >= test_start].drop(FEATURES_TO_DROP, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1645af8f-001a-4556-b7d3-b00df5fb39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits_expanding = split_by_equal_days(\n",
    "#     dt=df[\"datetime\"],\n",
    "#     train_days=365,\n",
    "#     fh_days=30,\n",
    "#     val_splits=val_splits,\n",
    "#     expanding=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f5e9a-0837-41e4-97c2-1f79e42ea490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_mae_2dlag = []\n",
    "\n",
    "# for split in range(VAL_SPLITS):\n",
    "#     val_df = df.loc[\n",
    "#         (df[\"datetime\"] >= splits_expanding[split][\"val\"][0])\n",
    "#         & (df[\"datetime\"] <= splits_expanding[split][\"val\"][1]),\n",
    "#         [\"target\", \"2d_lag_target\"],\n",
    "#     ]\n",
    "#     val_mae_2dlag.append(MAE(val_df[\"target\"], val_df[\"2d_lag_target\"]))\n",
    "# else:\n",
    "#     test_df = df.loc[df[\"datetime\"] >= test_start]\n",
    "#     val_mae_2dlag.append(MAE(test_df[\"target\"], test_df[\"2d_lag_target\"]))\n",
    "\n",
    "# print(\"Validation MAE:\", np.round(val_mae_2dlag, 3))\n",
    "# print(\"Validation mean MAE:\", np.round(np.mean(val_mae_2dlag), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aefefb-2912-4136-bff4-a567e5e2cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"datetime\"].is_monotonic_increasing\n",
    "# df = df.sort_values('datetime', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7078c-1a9f-4dfc-b573-61d229271cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d123e-d999-4ef1-8652-fcb541bda47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-01-24 00:00:00')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df[\"datetime\"].min() + (df[\"datetime\"].max() - df[\"datetime\"].min()) * 0.8).normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4b428-1f58-42d0-9afd-cf2e5702f13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-01-25 00:00:00')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[round(df.index[-1] * 0.8)][\"datetime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06747c39-53ff-40e3-a5b6-ea9538a00517",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_fixed = split_by_equal_days(\n",
    "    dt=df[\"datetime\"],\n",
    "    train_days=365 + 30 * 3,\n",
    "    fh_days=1,\n",
    "    val_splits=VAL_SPLITS,\n",
    "    expanding=False,\n",
    ")\n",
    "splits_fixed.extend(\n",
    "    split_by_equal_days(\n",
    "        dt=df[\"datetime\"],\n",
    "        train_days=365 + 30 * 6,\n",
    "        fh_days=1,\n",
    "        val_splits=VAL_SPLITS,\n",
    "        expanding=False,\n",
    "    )\n",
    ")\n",
    "# splits_fixed.extend(\n",
    "#     split_by_equal_days(\n",
    "#         dt=df[\"datetime\"],\n",
    "#         train_days=(\n",
    "#             df.datetime.max() - df.datetime.min() - Timedelta(days=30)\n",
    "#         ).days,  # all data except last 30 days\n",
    "#         fh_days=1,\n",
    "#         val_splits=VAL_SPLITS,\n",
    "#         expanding=False,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "new_splits = []\n",
    "for i, d in enumerate(splits_fixed):\n",
    "    count = len(\n",
    "        df.loc[\n",
    "            (df[\"datetime\"] >= d[\"val\"][0]) & (df[\"datetime\"] <= d[\"val\"][1])\n",
    "        ]\n",
    "    )\n",
    "    if count < 24:\n",
    "        print(f\"Split {i} only has {count} rows, removed\")\n",
    "    else:\n",
    "        new_splits.append(d)\n",
    "\n",
    "splits_fixed = new_splits\n",
    "# for i, d in enumerate(splits_fixed):\n",
    "#     print(i, \"train\", d[\"train\"])\n",
    "#     print(i, \"valid\", d[\"val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300cf217-7447-49fc-bbec-73a73bc07b4a",
   "metadata": {},
   "source": [
    "## Baselines comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea4a5b-8837-4823-85c4-c04e0eca796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(include=\"category\").columns.tolist()\n",
    "\n",
    "xgb_mae = []\n",
    "lgbm_mae = []\n",
    "cb_mae = []\n",
    "\n",
    "xgb_p = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 7,\n",
    "    \"random_state\": RAND,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"objective\": \"reg:absoluteerror\",\n",
    "    \"enable_categorical\": True,\n",
    "    \"early_stopping_rounds\": 20,\n",
    "    \"eval_metric\": \"mae\",\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "lgbm_p = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 7,\n",
    "    \"random_state\": RAND,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"objective\": \"regression_l1\",\n",
    "    \"metric\": \"mae\",\n",
    "    \"n_jobs\": -1,\n",
    "    \"force_col_wise\": True,\n",
    "    \"verbosity\": -1,\n",
    "}\n",
    "cb_p = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 7,\n",
    "    \"random_state\": RAND,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bylevel\": 0.8,\n",
    "    \"loss_function\": \"MAE\",\n",
    "    \"cat_features\": cat_cols,\n",
    "    \"eval_metric\": \"MAE\",\n",
    "    \"thread_count\": -1,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "xgbr = XGBRegressor(**xgb_p)\n",
    "lgbmr = LGBMRegressor(**lgbm_p)\n",
    "cbr = CatBoostRegressor(**cb_p)\n",
    "\n",
    "for model in [\n",
    "    xgbr,\n",
    "    lgbmr,\n",
    "    cbr,\n",
    "]:\n",
    "    for i, split in enumerate(splits_fixed):\n",
    "        df_train = df[\n",
    "            (df[\"datetime\"] >= split[\"train\"][0])\n",
    "            & (df[\"datetime\"] <= split[\"train\"][1])\n",
    "        ].drop(FEATURES_TO_DROP, axis=1)\n",
    "        df_val = df[\n",
    "            (df[\"datetime\"] >= split[\"val\"][0])\n",
    "            & (df[\"datetime\"] <= split[\"val\"][1])\n",
    "        ].drop(FEATURES_TO_DROP, axis=1)\n",
    "\n",
    "        X_train, y_train = (\n",
    "            df_train.drop([\"target\"], axis=1),\n",
    "            df_train[\"target\"],\n",
    "        )\n",
    "        X_val, y_val = df_val.drop([\"target\"], axis=1), df_val[\"target\"]\n",
    "        # X_test, y_test = df_test.drop([\"target\"], axis=1), df_test[\"target\"]\n",
    "\n",
    "        eval_set = [\n",
    "            (X_train, y_train),\n",
    "            (X_val, y_val),\n",
    "            # (X_test, y_test)\n",
    "        ]\n",
    "\n",
    "        # print(f\"{i+1} split\")\n",
    "        # print(\"-\" * 30)\n",
    "        # print(\"train\")\n",
    "        # print(\"start:\", split[\"train\"][0])\n",
    "        # print(\"end:\", split[\"train\"][1])\n",
    "        # print(\"shape:\", X_train.shape, y_train.shape)\n",
    "        # print(\"-\" * 30)\n",
    "        # print(\"val\")\n",
    "        # print(\"start:\", split[\"val\"][0])\n",
    "        # print(\"end:\", split[\"val\"][1])\n",
    "        # print(\"shape:\", X_val.shape, y_val.shape)\n",
    "        # print(\"-\" * 30)\n",
    "\n",
    "        # print(\"test\")\n",
    "        # print(\"start:\", test_start)\n",
    "        # print(\"end:\", test_end)\n",
    "        # print(\"shape:\", X_test.shape, y_test.shape)\n",
    "        # print(\"-\" * 30)\n",
    "\n",
    "        if isinstance(model, XGBRegressor):\n",
    "            # print(\"XGBRegressor\")\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                verbose=0,\n",
    "                # verbose=25,\n",
    "            )\n",
    "            xgb_mae.append(model.evals_result())\n",
    "\n",
    "        elif isinstance(model, LGBMRegressor):\n",
    "            # print(\"LGBMRegressor\")\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                categorical_feature=cat_cols,\n",
    "                eval_set=eval_set,\n",
    "                callbacks=[\n",
    "                    early_stopping(\n",
    "                        stopping_rounds=20,\n",
    "                        verbose=0,\n",
    "                    ),\n",
    "                    # log_evaluation(period=25),\n",
    "                ],\n",
    "            )\n",
    "            lgbm_mae.append(model.evals_result_)\n",
    "\n",
    "        elif isinstance(model, CatBoostRegressor):\n",
    "            # print(\"CatBoostRegressor\")\n",
    "            model.fit(\n",
    "                X_train, y_train, eval_set=eval_set, early_stopping_rounds=20\n",
    "            )\n",
    "            cb_mae.append(model.evals_result_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c727e-5125-4dfa-9bdd-db0b9870972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4704c16-0585-4e08-8c96-97a1aed5f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_mae[0]\n",
    "# lgbm_mae[0]\n",
    "# cb_mae[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b919e-611f-413c-8875-186ce3964934",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splits_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed314c1-85ca-4dfe-b286-771d7f8da989",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_splits = len(splits_fixed)\n",
    "rows, cols = 2, VAL_SPLITS\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(14, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for split_i in range(total_splits):\n",
    "    ax = axs[split_i]\n",
    "    ax.plot(xgb_mae[split_i][\"validation_1\"][\"mae\"], label=f\"XGB\")\n",
    "    ax.plot(lgbm_mae[split_i][\"valid_1\"][\"l1\"], label=f\"LGBM\")\n",
    "    ax.plot(cb_mae[split_i][\"validation_1\"][\"MAE\"], label=f\"CB\")\n",
    "\n",
    "    if split_i >= total_splits - cols:\n",
    "        ax.set_xlabel(\"Number of estimators\", size=9)\n",
    "    if split_i in [i * cols for i in range(rows)]:\n",
    "        ax.set_ylabel(\"MAE\", size=9)\n",
    "    ax.set_title(f\"{split_i + 1} split\", size=10)\n",
    "    ax.grid(which=\"both\", alpha=0.5)\n",
    "\n",
    "plt.legend(\n",
    "    loc=\"upper left\",\n",
    "    bbox_to_anchor=(1, 2.2),\n",
    "    frameon=False,\n",
    ")\n",
    "fig.suptitle(\n",
    "    \"MAE for all models\",\n",
    "    y=0.925,\n",
    "    fontsize=12,\n",
    ")\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b256a5-b461-4bb3-931e-f9515910274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_results = [\n",
    "#     xgb_mae[i][\"validation_1\"][\"mae\"][-1] for i in range(len(xgb_mae))\n",
    "# ]\n",
    "# lgbm_results = [lgbm_mae[i][\"valid_1\"][\"l1\"][-1] for i in range(len(lgbm_mae))]\n",
    "# cb_results = [\n",
    "#     cb_mae[i][\"validation_1\"][\"MAE\"][-1] for i in range(len(lgbm_mae))\n",
    "# ]\n",
    "\n",
    "# print(\"xgb mae:\", np.round(np.mean(xgb_results), 3))\n",
    "# print(\"lgbm mae:\", np.round(np.mean(lgbm_results), 3))\n",
    "# print(\"cb mae:\", np.round(np.mean(cb_results), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254887a-27c3-4adc-b796-6ea802c2ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for split in [[xgb_mae[i][\"validation_1\"][\"mae\"][-1] for i in range(split * 5, split * 5 + 5)] for split in range(4)]:\n",
    "#     print(np.round(np.mean(split), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6b0a3-f3f0-4cb5-bec4-c9fd6eca004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [[lgbm_mae[i][\"valid_1\"][\"l1\"][-1] for i in range(split * 5, split * 5 + 5)] for split in range(4)]:\n",
    "    print(np.round(np.mean(split), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6efd6-e06d-44c1-aeb3-b7500ae8b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [[cb_mae[i][\"validation_1\"][\"MAE\"][-1] for i in range(split * 5, split * 5 + 5)] for split in range(4)]:\n",
    "    print(np.round(np.mean(split), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4285ef46-2f51-4ad6-8625-fad3214a588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[i for i in range(split * 5, split * 5 + 5)] for split in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20064da-6d29-43ff-baab-fbdbac96770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# residuals = y_test - y_pred\n",
    "\n",
    "# plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "# plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Residuals\")\n",
    "# plt.title(\"Residuals vs Predicted\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa79087-4e06-48a8-bf03-2fbfdd1fcf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fi = pd.Series(xgbr.feature_importances_, index=X_train.columns)\n",
    "# fi = fi.sort_values(ascending=False).head(20)\n",
    "\n",
    "# fi.plot.barh(figsize=(8,6))\n",
    "# plt.xlabel(\"Feature Importance\")\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.title(\"Top 20 Important Features\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
