{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b39637c3-8ade-4574-9cb8-b58ed0231abe",
   "metadata": {},
   "source": [
    "# 1. Description\n",
    "Data description sourced from the Kaggle competition page.\n",
    "\n",
    "### train.csv\n",
    "- `county` - An ID code for the county.\n",
    "- `is_business` - Boolean for whether or not the prosumer is a business.\n",
    "- `product_type` - ID code with the following mapping of codes to contract types: `{0: \"Combined\", 1: \"Fixed\", 2: \"General service\", 3: \"Spot\"}`.\n",
    "- `target` - The consumption or production amount for the relevant segment for the hour. The segments are defined by the `county`, `is_business`, and `product_type`.\n",
    "- `is_consumption` - Boolean for whether or not this row's target is consumption or production.\n",
    "- `datetime` - The Estonian time in EET (UTC+2) / EEST (UTC+3). It describes the start of the 1-hour period on which target is given.\n",
    "- `data_block_id` - All rows sharing the same `data_block_id` will be available at the same forecast time. This is a function of what information is available when forecasts are actually made, at 11 AM each morning. For example, if the forecast weather `data_block_id` for predictins made on October 31st is 100 then the historic weather `data_block_id` for October 31st will be 101 as the historic weather data is only actually available the next day.\n",
    "- `row_id` - A unique identifier for the row.\n",
    "- `prediction_unit_id` - A unique identifier for the `county`, `is_business`, and `product_type` combination. *New prediction units can appear or disappear in the test set*.\n",
    "\n",
    "### gas_prices.csv\n",
    "- `origin_date` - The date when the day-ahead prices became available.\n",
    "- `forecast_date` - The date when the forecast prices should be relevant.\n",
    "- `[lowest/highest]_price_per_mwh` - The lowest/highest price of natural gas that on the day ahead market that trading day, in Euros per megawatt hour equivalent.\n",
    "- `data_block_id`\n",
    "\n",
    "### client.csv\n",
    "- `product_type`\n",
    "- `county` - An ID code for the county. See `county_id_to_name_map.json` for the mapping of ID codes to county names.\n",
    "- `eic_count` - The aggregated number of consumption points (EICs - European Identifier Code).\n",
    "- `installed_capacity` - Installed photovoltaic solar panel capacity in kilowatts.\n",
    "- `is_business` - Boolean for whether or not the prosumer is a business.\n",
    "- `date`\n",
    "- `data_block_id`\n",
    "\n",
    "### electricity_prices.csv\n",
    "- `origin_date`\n",
    "- `forecast_date` - Represents the start of the 1-hour period when the price is valid.\n",
    "- `euros_per_mwh` - The price of electricity on the day ahead markets in euros per megawatt hour.\n",
    "- `data_block_id`\n",
    "\n",
    "### forecast_weather.csv\n",
    "Weather forecasts that would have been available at prediction time. Sourced from the [European Centre for Medium-Range Weather Forecasts](https://codes.ecmwf.int/grib/param-db/?filter=grib2).\n",
    "\n",
    "- `[latitude/longitude]` - The coordinates of the weather forecast.\n",
    "- `origin_datetime` - The timestamp of when the forecast was generated.\n",
    "- `hours_ahead` - The number of hours between the forecast generation and the forecast weather. Each forecast covers 48 hours in total.\n",
    "- `temperature` - The air temperature at 2 meters above ground in degrees Celsius. Estimated for the end of the 1-hour period.\n",
    "- `dewpoint` - The dew point temperature at 2 meters above ground in degrees Celsius. Estimated for the end of the 1-hour period.\n",
    "- `cloudcover_[low/mid/high/total]` - The percentage of the sky covered by clouds in the following altitude bands: 0-2 km, 2-6, 6+, and total. Estimated for the end of the 1-hour period.\n",
    "- `10_metre_[u/v]_wind_component` - The [eastward/northward] component of wind speed measured 10 meters above surface in meters per second. Estimated for the end of the 1-hour period.\n",
    "- `data_block_id`\n",
    "- `forecast_datetime` - The timestamp of the predicted weather. Generated from `origin_datetime` plus `hours_ahead`. This represents the start of the 1-hour period for which weather data are forecasted.\n",
    "- `direct_solar_radiation` - The direct solar radiation reaching the surface on a plane perpendicular to the direction of the Sun accumulated during the hour, in watt-hours per square meter.\n",
    "- `surface_solar_radiation_downwards` - The solar radiation, both direct and diffuse, that reaches a horizontal plane at the surface of the Earth, accumulated during the hour, in watt-hours per square meter.\n",
    "- `snowfall` - Snowfall over hour in units of meters of water equivalent.\n",
    "- `total_precipitation` - The accumulated liquid, comprising rain and snow that falls on Earth's surface over the described hour, in units of meters.\n",
    "\n",
    "### historical_weather.csv\n",
    "[Historic weather data](https://open-meteo.com/en/docs).\n",
    "- `datetime` - This represents the start of the 1-hour period for which weather data are measured.\n",
    "- `temperature` - Measured at the end of the 1-hour period.\n",
    "- `dewpoint` - Measured at the end of the 1-hour period.\n",
    "- `rain` - Different from the forecast conventions. The rain from large scale weather systems of the hour in millimeters.\n",
    "- `snowfall` - Different from the forecast conventions. Snowfall over the hour in centimeters.\n",
    "- `surface_pressure` - The air pressure at surface in hectopascals.\n",
    "- `cloudcover_[low/mid/high/total]` - Different from the forecast conventions. Cloud cover at 0-3 km, 3-8, 8+, and total.\n",
    "- `windspeed_10m` - Different from the forecast conventions. The wind speed at 10 meters above ground in meters per second.\n",
    "- `winddirection_10m` - Different from the forecast conventions. The wind direction at 10 meters above ground in degrees.\n",
    "- `shortwave_radiation` - Different from the forecast conventions. The global horizontal irradiation in watt-hours per square meter.\n",
    "- `direct_solar_radiation`\n",
    "- `diffuse_radiation` - Different from the forecast conventions. The diffuse solar irradiation in watt-hours per square meter.\n",
    "- `[latitude/longitude]` - The coordinates of the weather station.\n",
    "- `data_block_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f34ecfd-d96f-43b1-b024-78ee799db55b",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4466c1-add1-44f4-8d21-2f19460c46fa",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8694e6f-4c39-4f4c-938a-d919b4fa6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import date, time\n",
    "\n",
    "import colorcet as cc\n",
    "import holidays\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "from matplotlib.dates import DateFormatter, MonthLocator\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from pandas.core.groupby.generic import DataFrameGroupBy\n",
    "from prophet import Prophet\n",
    "from scipy.ndimage import binary_dilation\n",
    "from shapely.geometry import shape\n",
    "from statsmodels.tsa.seasonal import MSTL, seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f25ca-5e4f-4031-a941-c597d861d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAND = 10\n",
    "RAW_DATA = \"../raw_data/\"\n",
    "\n",
    "train_df = pd.read_csv(f\"{RAW_DATA}train.csv\")\n",
    "gas_prices_df = pd.read_csv(f\"{RAW_DATA}gas_prices.csv\")\n",
    "client_df = pd.read_csv(f\"{RAW_DATA}client.csv\")\n",
    "electricity_prices_df = pd.read_csv(f\"{RAW_DATA}electricity_prices.csv\")\n",
    "forecast_weather_df = pd.read_csv(f\"{RAW_DATA}forecast_weather.csv\")\n",
    "historical_weather_df = pd.read_csv(f\"{RAW_DATA}historical_weather.csv\")\n",
    "station_county_mapping = pd.read_csv(\n",
    "    f\"{RAW_DATA}weather_station_to_county_mapping.csv\"\n",
    ")\n",
    "county_id_to_name_map = pd.read_json(\n",
    "    f\"{RAW_DATA}county_id_to_name_map.json\",\n",
    "    typ=\"series\",\n",
    ").str.lower()\n",
    "\n",
    "# External file with Estonian counties boundaries for visualisation\n",
    "with open(\"../additional_data/estonia.geojson\", \"r\", encoding=\"utf-8\") as f:\n",
    "    estonia_geojson = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f971b-a00d-4c26-a23c-dfdeb274f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_DICT = {\n",
    "    \"county\": county_id_to_name_map,\n",
    "    \"is_business\": {0: \"not_business\", 1: \"business\"},\n",
    "    \"is_consumption\": {0: \"production\", 1: \"consumption\"},\n",
    "    \"product_type\": {\n",
    "        0: \"combined\",\n",
    "        1: \"fixed\",\n",
    "        2: \"general_service\",\n",
    "        3: \"spot\",\n",
    "    },\n",
    "}\n",
    "\n",
    "PALETTE = sns.color_palette(\n",
    "    cc.glasbey[:4]\n",
    "    + [cc.glasbey[8]]\n",
    "    + cc.glasbey[5:8]\n",
    "    + [cc.glasbey[4]]\n",
    "    + [cc.glasbey[12]]\n",
    "    + cc.glasbey[10:12]\n",
    "    + [cc.glasbey[9]]\n",
    "    + cc.glasbey[13:16]\n",
    ").as_hex()\n",
    "\n",
    "SEGMENT_C = [\"county\", \"product_type\", \"is_business\"]\n",
    "CATEGORICAL_C = [\"county\", \"product_type\", \"is_business\", \"is_consumption\"]\n",
    "TARGET_C = [\n",
    "    \"county\",\n",
    "    \"product_type\",\n",
    "    \"is_business\",\n",
    "    \"is_consumption\",\n",
    "    \"datetime\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e679b2-edac-46b9-b48a-5717b737aa36",
   "metadata": {},
   "source": [
    "## Display Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a70352-098d-4da4-aae9-3522943f0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\n",
    "    \"display.float_format\",\n",
    "    lambda x: f\"{x:.2e}\" if abs(x) < 0.01 and x != 0 else f\"{x:.2f}\",\n",
    ")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67c46a-8dd3-4bec-87b8-001cfef3a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4a7c1b-4e48-40e5-bf3b-a13b50b532d4",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9f1f9d-7938-4863-a3ee-85b470c4deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_right_filled_header(text: str) -> None:\n",
    "    \"\"\"\n",
    "    Prints the provided header text followed by \">\" characters, filling\n",
    "    the line up to 100 characters.\n",
    "    \"\"\"\n",
    "    total_length = 100\n",
    "    text = text.strip()\n",
    "    print(\"\\n\" + text + \" \" + \">\" * (total_length - len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d008594e-8f65-4e76-a9bb-82ae23bb6650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_size(df: pd.DataFrame) -> np.int64:\n",
    "    \"Return total size of the dataframe in megabytes.\"\n",
    "    return df.memory_usage().sum() // (1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d7f167-e308-4364-bec0-169c0f275738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarize a dataframe with combined information from df.info() and\n",
    "    df.describe(). Additionally, calculate the maximum length of the\n",
    "    decimal part for numpy.float64 and numpy.float32 columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input dataframe to summarize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A combined summary dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    column_info = {\n",
    "        \"column\": df.columns,\n",
    "        \"not_na_count\": [],\n",
    "        \"dtype\": df.dtypes.values,\n",
    "        \"size_mb\": (df.memory_usage().div(1024**2).round(2))[1:],\n",
    "        \"max_dec_len\": [],\n",
    "    }\n",
    "\n",
    "    for column in df.columns:\n",
    "        col_data = df[column]\n",
    "        column_info[\"not_na_count\"].append(col_data.notna().sum())\n",
    "\n",
    "        if (not isinstance(col_data.dtype, pd.CategoricalDtype)) and (\n",
    "            np.issubdtype(col_data.dtype, np.float64)\n",
    "            or np.issubdtype(col_data.dtype, np.float32)\n",
    "        ):\n",
    "            decimal_len = (\n",
    "                col_data.dropna()\n",
    "                .apply(lambda x: len(str(x).split(\".\")[1]))\n",
    "                .max()\n",
    "            )\n",
    "            column_info[\"max_dec_len\"].append(decimal_len)\n",
    "        else:\n",
    "            column_info[\"max_dec_len\"].append(-1)\n",
    "\n",
    "    df = pd.merge(\n",
    "        pd.DataFrame(\n",
    "            data=column_info,\n",
    "        ),\n",
    "        df.describe(include=\"all\").drop(\"count\").T.reset_index(names=\"column\"),\n",
    "        on=[\"column\"],\n",
    "    )\n",
    "\n",
    "    first_columns = [\n",
    "        \"column\",\n",
    "        \"dtype\",\n",
    "        \"size_mb\",\n",
    "        \"max_dec_len\",\n",
    "        \"min\",\n",
    "        \"max\",\n",
    "    ]\n",
    "    other_columns = [\n",
    "        column for column in df.columns if column not in first_columns\n",
    "    ]\n",
    "\n",
    "    return df.loc[:, first_columns + other_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22058a79-8e71-4372-9b74-a4f77b4c2107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_show_info(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Print the number of dataframe NaNs, the count of duplicated rows\n",
    "    and display a summary of the dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    print_right_filled_header(\"HEAD\")\n",
    "    display(df.head())\n",
    "\n",
    "    print_right_filled_header(\"TAIL\")\n",
    "    display(df.tail())\n",
    "\n",
    "    print_right_filled_header(\"MISSING VALUES\")\n",
    "    nan_counts = df.isna().sum()\n",
    "    nan_series = nan_counts[nan_counts > 0]\n",
    "    if nan_series.empty:\n",
    "        print(\"No missing values.\")\n",
    "    else:\n",
    "        display(nan_series)\n",
    "\n",
    "    print_right_filled_header(\"FULL DUPLICATES\")\n",
    "    dup_counts = df.duplicated().sum()\n",
    "    if dup_counts:\n",
    "        display(f\"Total duplicated rows: {dup_counts}\")\n",
    "    else:\n",
    "        print(\"No duplicate rows.\")\n",
    "\n",
    "    print_right_filled_header(\"TOTAL SIZE\")\n",
    "    print(f\"Total DataFrame size: {total_size(df)} MB.\")\n",
    "\n",
    "    print_right_filled_header(\"SUMMARY\")\n",
    "    display(columns_summary(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c2409-d07e-4233-8f03-e87f200726df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_mapper(\n",
    "    df: pd.DataFrame,\n",
    "    values_mapper: dict,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Map categorical columns in the dataframe to their corresponding\n",
    "    values based on the provided mapper and convert these columns to\n",
    "    categorical type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe to process.\n",
    "    values_mapper : dict\n",
    "        A dictionary where keys are column names, and values are\n",
    "        mapping dictionaries.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Modified dataframe with specified columns mapped to their\n",
    "        categorical values.\n",
    "    \"\"\"\n",
    "    for key, value in values_mapper.items():\n",
    "        if key in df.columns:\n",
    "            df[key] = df[key].map(value).astype(\"category\")\n",
    "        else:\n",
    "            print(\n",
    "                \"categorical_mapper:\",\n",
    "                f\"column '{key}' not found in dataframe, skipping.\"\n",
    "            )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec08694b-0ac5-4b58-99e2-e9231b4a4cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mismatched_indices_check(\n",
    "    date_diff: pd.Series,\n",
    "    id_diff: pd.Series,\n",
    "    difference: np.float64,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Check if rows with an n-day difference in the date_diff have\n",
    "    mismatched indices with id_diff rows that have a difference of n\n",
    "    and print it.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\n",
    "        f\"Difference {difference}: \",[\n",
    "            \"no mismatches\",\n",
    "            \"indices mismatch detected\"][\n",
    "        int(np.any(\n",
    "            date_diff[date_diff == pd.Timedelta(difference, \"day\")]\n",
    "            .index\n",
    "            != id_diff[id_diff == difference].index))],\n",
    "        \".\",\n",
    "        sep=\"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c11981-b592-495d-8765-2cf853d62efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_groups(dfgb: DataFrameGroupBy, column: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Combine values of the specified column in each subgroup into\n",
    "    tuples, compare these tuples within each group, and return a\n",
    "    pd.Series of boolean values indicating whether each tuple is a duplicate.\n",
    "    \"\"\"\n",
    "    return dfgb[column].agg(tuple).duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0854a-1cf5-4f6f-af67-f220ea830592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circle_label(color: tuple | str, label: str) -> Line2D:\n",
    "    \"\"\"\n",
    "    Create a handle for legend with a circular marker with specified\n",
    "    color and label.\n",
    "    \"\"\"\n",
    "    return Line2D(\n",
    "        [],\n",
    "        [],\n",
    "        color=color,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"None\",\n",
    "        markersize=8,\n",
    "        label=label,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989cbfe-a597-461b-8fb9-70beb445d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_combinations(df: pd.DataFrame, combination: list) -> pd.DataFrame:\n",
    "    return (\n",
    "        df[combination]\n",
    "        .drop_duplicates()\n",
    "        .sort_values(combination, ignore_index=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1a812c-5e41-4b2c-9cbf-1ea03ad7a37a",
   "metadata": {},
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d43a7e-c58e-4e91-9ee4-aa1c401af124",
   "metadata": {},
   "source": [
    "## Feature Commentary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9e7dc-7191-4882-8da4-58fe3030a175",
   "metadata": {},
   "source": [
    "From description:\n",
    "- `datetime` - The Estonian time in EET (UTC+2) / EEST (UTC+3).\n",
    "- `data_block_id` - All rows sharing the same `data_block_id` will be available at the same forecast time. This is a function of what information is available when forecasts are actually made, at 11 AM each morning. For example, if the forecast weather `data_block_id` for predictins made on October 31st is 100 then the historic weather `data_block_id` for October 31st will be 101 as the historic weather data is only actually available the next day.\n",
    "- `row_id` - A unique identifier for the row.\n",
    "- `prediction_unit_id` - A unique identifier for the `county`, `is_business`, and `product_type` combination. *New prediction units can appear or disappear in the test set*.\n",
    "***\n",
    "Also, the competition host <u>[provided](https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers/discussion/455833)</u> this scheme along with comments:\n",
    "\n",
    "Letâ€™s say we are on day D at 11am. We want to predict next day D+1 net consumption from 00 to 23 for every hours.\n",
    "<table style=\"border: 1px solid black; border-collapse: collapse; width: 100%;\">\n",
    "  <tr>\n",
    "    <th style=\"border: 1px solid black;\">Category</th>\n",
    "    <th style=\"border: 1px solid black;\">Weather forecast</th>\n",
    "    <th style=\"border: 1px solid black;\">Historical weather</th>\n",
    "    <th style=\"border: 1px solid black;\">Historical consumption and production / Client data</th>\n",
    "    <th style=\"border: 1px solid black;\">Electricity prices</th>\n",
    "    <th style=\"border: 1px solid black;\">Gas prices</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"border: 1px solid black;\">Last available data</td>\n",
    "    <td style=\"border: 1px solid black;\">Forecast for every hours of D and D+1 (published on D)</td>\n",
    "    <td style=\"border: 1px solid black;\">Every hours until day D, 10 am</td>\n",
    "    <td style=\"border: 1px solid black;\">Every hours of Day D-1</td>\n",
    "    <td style=\"border: 1px solid black;\">Every hours of day D (published on D-1)</td>\n",
    "    <td style=\"border: 1px solid black;\">Data for day D (published on D-1)</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Prices are published everyday at 2 pm (so after 11 am), that is we do not have D+1 prices.\n",
    "The data_block_id already reflects this timeline of availability of the data. There is no need to apply additional lag if joining on data_block_id.\n",
    "***\n",
    "Electricity and gas prices, dates, and `data_block_id` were additionally <u>[clarified](https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers/discussion/456636#2533090)</u>:\n",
    "\n",
    "> Q: Are the values in the gas & electricity data actual values or predicted?\n",
    "\n",
    "> A: Actual values. Prices are published everyday for the next day. However we predict electricity consumption and production earlier, so next day's prices are not yet available when predicting next day's production and consumption\n",
    "***\n",
    "Later, I will explore whether there are any discrepancies in the correlations between `data_block_id` and `datetime`, or between `prediction_unit_id` and the `categorical features`. For now, I will analyze the data by:\n",
    "1. Checking all dataframes using `describe()` and `info()`.\n",
    "2. Verifying the presence of missing values or duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d370cf86-2a74-4eb8-ae1a-e76832270193",
   "metadata": {},
   "source": [
    "## train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab26fa8d-480e-4216-814e-186a8ba13a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_show_info(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7276e470-db61-40b8-b140-9d06e4661c6a",
   "metadata": {},
   "source": [
    "- There are a few missing values (528/2,018,352 < 0.1%) and no duplicates in the dataframe. Only the `target` feature has missing values.\n",
    "- The categorical features `county`, `is_business`, `product_type`, and `is_consumption` can be converted to the categorical data type. Their values can be renamed to improve clarity and understanding.\n",
    "- The `target` feature can be converted to `float32` because its values range from 0.0 to 15,480.27, and the standard deviation (910.20) is relatively large compared to the mean (274.86), indicating a wide distribution of values. Given the dataset size (2,017,824 records), the precision of the fractional part becomes less significant, and `float32` provides adequate precision while reducing memory usage.\n",
    "- The `datetime` feature can be converted to `datetime64[ns]` for proper handling of data.\n",
    "- The `data_block_id` feature can be converted to `uint16` (range: 0 through 65,535) as its maximum value is small.\n",
    "- The `row_id` feature is similar to the index values in the current default sorting. For now, it can be converted to `uint32` (range: 0 through 4,294,967,295) as its range fits well within this data type, and the use of `int64` is unnecessary because it supports negative numbers, which are irrelevant in this case.\n",
    "- The `prediction_unit_id` feature can be converted to `uint8` (range: 0 through 255). Although new combinations may appear in the future, this data type is sufficient to uniquely represent all future combinations of current county, is_business, and product_type (16 * 2 * 4 = 96)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116376df-986a-4875-bbaf-cea7a4691065",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aea92f-b811-4ee0-ac90-f6a60de0b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "train_df = train_df[\n",
    "    [\n",
    "        \"county\",\n",
    "        \"product_type\",\n",
    "        \"is_business\",\n",
    "        \"is_consumption\",\n",
    "        \"datetime\",\n",
    "        \"target\",\n",
    "        \"data_block_id\",\n",
    "        \"row_id\",\n",
    "        \"prediction_unit_id\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Rename categorical values to avoid confusion and improve readability\n",
    "train_df = categorical_mapper(train_df, CATEGORICAL_DICT)\n",
    "\n",
    "# Change data types to reduce memory usage\n",
    "train_df = train_df.astype(\n",
    "    {\n",
    "        \"target\": \"float32\",\n",
    "        \"data_block_id\": \"uint16\",\n",
    "        \"row_id\": \"uint32\",\n",
    "        \"prediction_unit_id\": \"uint8\",\n",
    "        \"datetime\": \"datetime64[ns]\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Size after transformation: {total_size(train_df)} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb4d9f-a83e-43ec-9f5e-64c838b4d95e",
   "metadata": {},
   "source": [
    "### Partial Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3337375-7df6-4c74-a561-278dc0257519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check timestamp and categorical features for duplicates\n",
    "train_df.duplicated(TARGET_C).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d79b3-926f-44a8-85d5-159cb20248e8",
   "metadata": {},
   "source": [
    "No partial duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a38920-b145-4834-b3ee-4e6d1c2cf9c8",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73cb832-7f59-466f-8bf8-c3f29cca04f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rows with missing values from the dataframe\n",
    "train_df[train_df.isna().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540205db-12b9-4799-9270-d659285802f8",
   "metadata": {},
   "source": [
    "The `datetime` values for rows with missing target values start from '2021-10-31 03:00:00'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abafc092-d591-48f0-b690-f6f042dd8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array with timestamps corresponding to rows with missing\n",
    "# target values\n",
    "na_datetimes = train_df[train_df.isna().any(axis=1)][\"datetime\"].unique()\n",
    "na_datetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5904abf-b0e6-4553-8d34-29876715901e",
   "metadata": {},
   "source": [
    "All missing values correspond to the start or end of daylight saving time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a879cdba-8ed4-4dc5-92e7-7ff9510f7b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all values for these timestamps are missing\n",
    "train_df.isna().values.sum() == train_df.loc[\n",
    "    train_df[\"datetime\"].isin(na_datetimes), [\"target\"]\n",
    "].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90089ba2-a3dc-4d77-8bc8-539ac3fe37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check entries near NaNs\n",
    "for timestamp in na_datetimes:\n",
    "    display(\n",
    "        train_df.loc[\n",
    "            (\n",
    "                # Loc on categories from the first row for simplicity\n",
    "                (\n",
    "                    train_df[CATEGORICAL_C] == train_df.iloc[0][CATEGORICAL_C]\n",
    "                ).all(axis=1)\n",
    "            )\n",
    "            & (\n",
    "                train_df[\"datetime\"].isin(\n",
    "                    pd.date_range(\n",
    "                        timestamp - pd.Timedelta(\"1 h\"),\n",
    "                        timestamp + pd.Timedelta(\"1 h\"),\n",
    "                        freq=\"h\",\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            [\"datetime\", \"target\"],\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6496441-199b-4f92-88de-130257f60e99",
   "metadata": {},
   "source": [
    "The data for the switch from DST is filled similarly to the switch to DST; however, these are two distinct processes with significant differences. The spring switch adds one hour, which results in a missing row with `NaN`. The autumn switch subtracts one hour, which would typically create a duplicated timestamp. However, in the provided data, no duplicate timestamps are present; instead, a row with `NaN` appears again.\n",
    "\n",
    "For example, the timestamp from 3-1 = 2 is duplicated (and is missing in the DataFrame), 4-1 = 3 results in a row with `NaN`, and only from 5-1 = 4 does the value appear as expected (not `NaN`).\n",
    "Because of this, it is acceptable to use interpolation to impute the missing value for the autumn switch, while simply dropping the missing value for the spring switch. Other methods may also be applicable, particularly if it is important to preserve the one-hour structure of the time series, as will be discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdf5320-128e-4337-8e99-2947aae538e6",
   "metadata": {},
   "source": [
    "### Consistency and Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21117f6d-8709-463e-b9ff-7d7434a1d967",
   "metadata": {},
   "source": [
    "#### Row Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165075d2-d32b-47c1-81d3-877f37ac378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if row_id values are equal to index values\n",
    "(train_df[\"row_id\"] != train_df.index).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c933c4-19d6-431c-a0d4-f573a5583a99",
   "metadata": {},
   "source": [
    "All `row_id` values are equal to index values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f742e-eed4-4ba5-898d-b681b80ee2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns=[\"row_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f0e13-ea49-40f3-bf67-f90ab25c624d",
   "metadata": {},
   "source": [
    "#### Datetime and data_block_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f268a-47c2-44a4-b0a0-ffb3b8e65b20",
   "metadata": {},
   "source": [
    "For predictions made for day D + 1, all historical consumption and production data should have `data_block_id` values equal to D - 1. This is because the `data_block_id` represents the data available at a specific time. Data for day D is unavailable at the time of prediction, as it corresponds to the current day, and no historical data exists for it yet.\n",
    "\n",
    "To validate this, the `datetime` and `data_block_id` columns require a check to ensure that, in chronological order, both date and ID values progress uniformly. If a single date corresponds to multiple `data_block_id` values, it will be evident because the differences between consecutive rows in these two columns will not be consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21aaedd-2da4-4966-880f-50304a7d8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables to store the differences between the current and\n",
    "# previous row for all rows in datetime and data_block_id\n",
    "train_date_diff = train_df[\"datetime\"].dt.date.diff()\n",
    "train_id_diff = train_df[\"data_block_id\"].diff()\n",
    "\n",
    "# Print unique values to verify that default order reflects sorting\n",
    "# from oldest to newest without skips\n",
    "print(train_date_diff.value_counts(dropna=False))\n",
    "print(train_id_diff.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec5185-1db8-4b0f-ad40-a02158da4c6f",
   "metadata": {},
   "source": [
    "- `NaT` and `NaN` values correspond to the first row in both columns, where no previous row data is available for comparison.\n",
    "- A value of '0' indicates that the current and previous rows belong to the same day.\n",
    "- A value of '1' indicates the transition from one day to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5effd833-7188-40e0-af43-7b4c5a2bf0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for difference in train_id_diff.unique():\n",
    "    mismatched_indices_check(train_date_diff, train_id_diff, difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5844145-bb47-4d99-9c87-f651c87667ee",
   "metadata": {},
   "source": [
    "The default order of the raw data for this dataframe is sorted by datetime, and the `data_block_id` reflects the actual data availability, with no errors observed in the `datetime` column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa400fb-dbde-48f2-b0a7-7f8dfac5a2ad",
   "metadata": {},
   "source": [
    "#### Time Series Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337259f-3439-42d2-ad5c-6334b67db13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_missing = train_df[TARGET_C + [\"target\"]].dropna().copy()\n",
    "\n",
    "# Add hour index starting from the beginning\n",
    "df_for_missing[\"hour_index\"] = (\n",
    "    (df_for_missing[\"datetime\"] - df_for_missing[\"datetime\"].min())\n",
    "    // pd.Timedelta(hours=1)\n",
    ").astype(np.uint16)\n",
    "\n",
    "# Add a group index corresponding to the unique combination of all\n",
    "# categorical features (i.e. this feature is not equal to\n",
    "# prediction_unit_id): county, is_business, product_type,\n",
    "# is_consumption.\n",
    "# The maximum number of combinations is 16 * 4 * 2 * 2 = 256, but the\n",
    "# actual number of observed values is less than 256\n",
    "df_for_missing[\"group_index\"] = (\n",
    "    df_for_missing.groupby(CATEGORICAL_C, observed=True,)\n",
    "    .ngroup()\n",
    "    .astype(np.uint8)\n",
    ")\n",
    "\n",
    "# Create a 2D array with a shape equal to the number of groups and the\n",
    "# total number of hours between the minimum and maximum timestamps\n",
    "n_groups = df_for_missing[\"group_index\"].nunique()\n",
    "n_hours = df_for_missing[\"hour_index\"].max() + 1\n",
    "missmap = np.full((n_groups, n_hours), np.nan)\n",
    "\n",
    "# Fill the array with the corresponding flag values\n",
    "missmap[df_for_missing[\"group_index\"], df_for_missing[\"hour_index\"]] = (\n",
    "    df_for_missing[\"target\"] != 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab4ffe7-6b8f-445b-aaac-cb2b5247c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dilate missing values because of high density of plot and isolated\n",
    "# missing values are not visible.\n",
    "n = 6 # How many hours before and after count as missing\n",
    "structure = np.ones((1, 2 * n + 1), dtype=bool)\n",
    "missing_dilated = binary_dilation(np.isnan(missmap), structure=structure)\n",
    "missmap[missing_dilated] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795401c-07a8-47f0-9f27-c31e79fd7f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "\n",
    "min_date = df_for_missing[\"datetime\"].min()\n",
    "max_date = df_for_missing[\"datetime\"].max()\n",
    "n_months = len(pd.date_range(start=min_date, end=max_date, freq=\"ME\"))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "sns.heatmap(\n",
    "    missmap,\n",
    "    cmap=sns.color_palette([\"tab:blue\", \"tab:orange\"]),\n",
    "    cbar=False,\n",
    "    alpha=alpha,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "xtick_indices = np.linspace(0, n_hours, num=n_months, dtype=int)\n",
    "xtick_labels = pd.date_range(min_date, max_date, freq=\"MS\")\n",
    "\n",
    "ax.set_xticks(xtick_indices)\n",
    "ax.set_xticklabels(xtick_labels.strftime(\"%Y-%m\"), rotation=45, ha=\"right\")\n",
    "\n",
    "dx, dy = 0, -2.5\n",
    "for label in ax.get_yticklabels():\n",
    "    offset = mt.ScaledTranslation(dx / 72, dy / 72, fig.dpi_scale_trans)\n",
    "    label.set_transform(label.get_transform() + offset)\n",
    "\n",
    "legend_patches = [\n",
    "    mpatches.Patch(\n",
    "        facecolor=\"white\",\n",
    "        label=\"Missing value\",\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "    ),\n",
    "    mpatches.Patch(\n",
    "        facecolor=\"tab:blue\",\n",
    "        alpha=alpha,\n",
    "        label=\"Zero value\",\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "    ),\n",
    "    mpatches.Patch(\n",
    "        facecolor=\"tab:orange\",\n",
    "        alpha=alpha,\n",
    "        label=\"Not zero value\",\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "    ),\n",
    "]\n",
    "\n",
    "plt.legend(\n",
    "    handles=legend_patches,\n",
    "    title=\"Data Presence\",\n",
    "    title_fontsize=12,\n",
    "    bbox_to_anchor=(1, 1),\n",
    "    loc=\"upper left\",\n",
    "    fontsize=11,\n",
    "    frameon=False,\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    \"Heatmap of time series gaps for all combinations \"\n",
    "    \"of categorical features\",\n",
    "    fontsize=13,\n",
    ")\n",
    "plt.xlabel(\"Month\", fontsize=10)\n",
    "plt.ylabel(\"Group index\", fontsize=10)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78336ebe-1ff8-4442-96d0-6349ffaa97c6",
   "metadata": {},
   "source": [
    "The plot shows that time series across different groups have varying start and end dates. Some groups have large gaps in their data, and the overall data volume varies between groups. Noticeably more zero values occur in winter than in summer, and these zeros appear every other hour - suggesting electricity production patterns. Missing values due to the switch to or from DST ([1 hour shift](#Missing-Values)) are displayed as vertical white lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb884e-139d-45f0-b097-00b97bfd7db1",
   "metadata": {},
   "source": [
    "#### Categorical Features Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abbc69f-e0a9-4365-9120-c2ea83e07d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that each combination of county, product_type, and\n",
    "# is_business corresponds to exactly one unique prediction_unit_id\n",
    "(\n",
    "    train_df.groupby(\n",
    "        SEGMENT_C,\n",
    "        observed=True,\n",
    "    )[\"prediction_unit_id\"].nunique()\n",
    "    != 1\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d8fe47-e778-438d-9512-c9adfc418174",
   "metadata": {},
   "source": [
    "All combinations of `county`, `is_business`, and `product_type` correspond to a single `prediction_unit_id`, with no errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbce70a-9fbc-4755-be05-7c7a2cebecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in CATEGORICAL_C:\n",
    "    print(train_df[c].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b2ad6-3950-45dc-967d-1e775a9a1076",
   "metadata": {},
   "source": [
    "Classes in each category except `is_consumption` are imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0acb100-3b78-4281-a922-08b34f9c0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that in each segment the number of production and consumptuon\n",
    "# values are the same\n",
    "train_df.groupby(CATEGORICAL_C, as_index=False, observed=True)[\n",
    "    \"target\"\n",
    "].count().groupby(SEGMENT_C, as_index=False, observed=True)[\n",
    "    \"target\"\n",
    "].nunique()[\n",
    "    \"target\"\n",
    "].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d7257e-24cd-465a-b6a9-d9c5e9acdc0b",
   "metadata": {},
   "source": [
    "Yes, in each segment (combination of `county`, `product_type`, `is_business`) the count of `target` entries is identical for `production` and `consumption`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc78cb8-03b5-4cc2-82f6-340c3b3a113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the visualisation of the number of entries for each unique\n",
    "# combination of categorical features\n",
    "\n",
    "# Create a DF with all segments and their counts of target values\n",
    "train_categories = train_df.groupby(\n",
    "    SEGMENT_C,\n",
    "    as_index=False,\n",
    "    observed=True,\n",
    ")[\"target\"].count()\n",
    "\n",
    "# Create columns with concatenated labels for segments\n",
    "train_categories[\"group\"] = (\n",
    "    train_categories[SEGMENT_C].astype(str).agg(\" \".join, axis=1)\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "sns.barplot(\n",
    "    data=train_categories,\n",
    "    x=\"group\",\n",
    "    y=\"target\",\n",
    "    hue=\"county\",\n",
    "    palette=PALETTE,\n",
    "    width=1,\n",
    "    linewidth=0.4,\n",
    ")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "labels = [label.capitalize() for label in labels]\n",
    "plt.legend(\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    "    title=\"County\",\n",
    "    title_fontsize=12,\n",
    "    bbox_to_anchor=(1.015, 1.1),\n",
    "    loc=\"upper left\",\n",
    "    borderaxespad=0,\n",
    "    markerscale=3,\n",
    "    frameon=False,\n",
    "    fontsize=11,\n",
    ")\n",
    "\n",
    "plt.title(\"Number of records for each segment\", fontsize=13)\n",
    "plt.grid(False)\n",
    "\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "ax.set_xticklabels(\n",
    "    train_categories[\"group\"].str.split(n=1).str[1],\n",
    "    rotation=90,\n",
    "    fontsize=10,\n",
    ")\n",
    "plt.xlabel(\"Group\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897ec9f-fbc2-4762-87f9-ed5e22f44ea3",
   "metadata": {},
   "source": [
    "The number of entries in each segment ranges from around 4,000 to 30,000, and the number of subgroups also varies across counties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70137672-8297-45a9-a357-3e0ff989d2ec",
   "metadata": {},
   "source": [
    "## gas_prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765f2c74-d559-458c-9a78-5c9e92138fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_show_info(gas_prices_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb7db5-4144-4ba4-8124-1e8794b88c25",
   "metadata": {},
   "source": [
    "- There are no missing values or duplicates in the dataframe.\n",
    "- The `data_block_id` values start from 1, unlike the `data_block_id` values in train_df, which start from 0. This difference exists because </u>[`[lowest/highest]_price_per_mwh` are end-of-day prices and aren't available in the late morning when forecasts are made](https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers/discussion/453355#2515054)</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d22ee4-948f-4d37-a90b-dfe11be0fa14",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308fc923-cb29-4610-9b8a-06cd1cc3925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "gas_prices_df = gas_prices_df[\n",
    "    [\n",
    "        \"origin_date\",\n",
    "        \"forecast_date\",\n",
    "        \"data_block_id\",\n",
    "        \"lowest_price_per_mwh\",\n",
    "        \"highest_price_per_mwh\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Change data types to reduce memory usage\n",
    "gas_prices_df = gas_prices_df.astype(\n",
    "    {\n",
    "        \"origin_date\": \"datetime64[ns]\",\n",
    "        \"forecast_date\": \"datetime64[ns]\",\n",
    "        \"data_block_id\": \"uint16\",\n",
    "        \"lowest_price_per_mwh\": \"float32\",\n",
    "        \"highest_price_per_mwh\": \"float32\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b6250-5489-43c9-ab95-73b1250bc282",
   "metadata": {},
   "source": [
    "### Partial Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d7cb8-a997-4ad0-b7f8-0868b1e321ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check timestamps for duplicates\n",
    "gas_prices_df.duplicated([\"forecast_date\", \"origin_date\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0972af6c-4366-4679-bdae-3c6d4a9df9f5",
   "metadata": {},
   "source": [
    "No partial duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c6c68f-f43b-4457-b4b2-caf1379641bc",
   "metadata": {},
   "source": [
    "### Consistency and Relationship Validation\n",
    "#### Origin and forecast dates\n",
    "Due to the strong correlation between the two datetime columns (at least based on the description and a few observed rows), it's necessary to verify if this correlation holds true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b8c99-8b05-4406-8fa4-a13ffe01b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that each origin date is the next day compared to the previous\n",
    "# one\n",
    "gas_date_diff = gas_prices_df[\"origin_date\"].diff()\n",
    "print(gas_date_diff.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac223506-6fec-481d-9eb8-bc8dfc36cd1b",
   "metadata": {},
   "source": [
    "- Yes, all values are chronological days without skips. The `NaT` value corresponds to the first date, where no previous date exists for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9254055c-50b3-4702-8057-0b5862e6b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any forecast dates are not exactly one day after origin dates\n",
    "(\n",
    "    gas_prices_df[\"origin_date\"] + pd.Timedelta(days=1)\n",
    "    != gas_prices_df[\"forecast_date\"]\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f40e67-c764-4985-b86d-904044194ddd",
   "metadata": {},
   "source": [
    "- All `origin_date` values are correct and represent the day before their corresponding `forecast_date` values. This means that correlation between these features is 1, and one of them can likely be deleted.\n",
    "- The raw data in this dataframe is sorted chronologically by datetime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516a83fe-b2d0-4909-96ea-d77ec3d3a3d9",
   "metadata": {},
   "source": [
    "#### Origin date and data_block_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f248ed1-f98b-4c91-aa71-3c3f73073720",
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_id_diff = gas_prices_df[\"data_block_id\"].diff()\n",
    "print(gas_id_diff.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e0e15-83ad-4f2f-8148-767be995779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for difference in gas_id_diff.unique():\n",
    "    mismatched_indices_check(gas_date_diff, gas_id_diff, difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522bb44a-1e9d-45fd-bf7f-33d8b473820b",
   "metadata": {},
   "source": [
    "- Similar to `origin_date`, the first value is missing, and all subsequent values have a difference of 1. Since the differences are consistent across both columns, it can be concluded that there are no errors between `origin_date` and `data_block_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05598f-23b8-4c4d-9348-195317dc2dba",
   "metadata": {},
   "source": [
    "## client_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de931d3-3a4d-450b-a38c-df9047d00dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_show_info(client_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed2c661-ba53-4858-b61b-fababe96d55f",
   "metadata": {},
   "source": [
    "- There are no missing values or duplicates in the dataframe.\n",
    "- The `data_block_id` values start from 2 instead of 0 or 1, as mentioned earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37133253-bdda-4121-a185-856ee26f650d",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71bd98-3e38-4c5f-97a4-bfd9c1b7e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "client_df = client_df[\n",
    "    [\n",
    "        \"county\",\n",
    "        \"product_type\",\n",
    "        \"is_business\",\n",
    "        \"date\",\n",
    "        \"eic_count\",\n",
    "        \"installed_capacity\",\n",
    "        \"data_block_id\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Rename categorical values to avoid confusion and improve readability\n",
    "client_df = categorical_mapper(client_df, CATEGORICAL_DICT)\n",
    "\n",
    "# Change data types to reduce memory usage\n",
    "client_df = client_df.astype(\n",
    "    {\n",
    "        \"date\": \"datetime64[ns]\",\n",
    "        \"eic_count\": \"uint32\",\n",
    "        \"installed_capacity\": \"float32\",\n",
    "        \"data_block_id\": \"uint16\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4779c6c-5ff0-4101-b5f2-3555cf66ee18",
   "metadata": {},
   "source": [
    "### Partial duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22194d1b-d1e1-48ac-910c-19b9495c3c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check timestamp and categorical features for duplicates\n",
    "client_df.duplicated([\"county\", \"product_type\", \"is_business\", \"date\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc091b86-f5a5-4507-aaee-e8254c3b28dc",
   "metadata": {},
   "source": [
    "No partial duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd668c-fb8c-40fd-9ba4-1b915b83d911",
   "metadata": {},
   "source": [
    "### Consistency and Relationship Validation\n",
    "#### Date and data_block_id\n",
    "As with the previous dataframes, it is necessary to validate the correlation between `date` and `data_block_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b63cf9-15d2-4f24-b2a0-1999d15b760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables to store the differences between the current and\n",
    "# previous row for all rows in date and data_block_id\n",
    "client_date_diff = client_df[\"date\"].dt.date.diff()\n",
    "client_id_diff = client_df[\"data_block_id\"].diff()\n",
    "\n",
    "\n",
    "# Print unique values to verify that default order reflects sorting\n",
    "# from oldest to newest without skips\n",
    "print(client_date_diff.value_counts(dropna=False))\n",
    "print(client_id_diff.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2dcd03-99b7-4f13-95f1-890aefd19bf5",
   "metadata": {},
   "source": [
    "- `NaT` and `NaN` values correspond to the first row in both columns, where no previous row data is available for comparison.\n",
    "- A value of '0' indicates that the current and previous rows belong to the same day.\n",
    "- A value of '1' indicates the transition from one day to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89eda7a-2f9b-44ca-ab7c-1ddc5cd526a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for difference in client_id_diff.unique():\n",
    "    mismatched_indices_check(client_date_diff, client_id_diff, difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae831531-752d-4ca8-902e-c428007961f0",
   "metadata": {},
   "source": [
    "- The default order of the raw data for this dataframe is sorted by datetime, and the `data_block_id` reflects the actual data availability, with no errors observed in the `date` column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d83cf3c-9401-442d-bae5-290c9e14e973",
   "metadata": {},
   "source": [
    "## electricity_prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c7771-33b8-4c10-b6b4-f031d455512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_show_info(electricity_prices_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e19efa-97a7-461f-a349-a1aaab1a2bc3",
   "metadata": {},
   "source": [
    "- There are no missing values or duplicates in the dataframe.\n",
    "- The `data_block_id` values start from 1.\n",
    "- The minimum value of the `euros_per_mwh` column is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f68560-19e4-4cd6-b567-db034d5735da",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de611edc-fc44-44a8-a3ed-ee333ebe07fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data types to reduce memory usage\n",
    "electricity_prices_df = electricity_prices_df.astype(\n",
    "    {\n",
    "        \"origin_date\": \"datetime64[ns]\",\n",
    "        \"forecast_date\": \"datetime64[ns]\",\n",
    "        \"euros_per_mwh\": \"float32\",\n",
    "        \"data_block_id\": \"uint16\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Datetime at which the forecast will be available for the model\n",
    "electricity_prices_df[\"electricity_datetime\"] = electricity_prices_df[\n",
    "    \"origin_date\"\n",
    "] + pd.Timedelta(2, \"d\")\n",
    "\n",
    "# Reorder columns\n",
    "electricity_prices_df = electricity_prices_df[\n",
    "    [\n",
    "        \"electricity_datetime\",\n",
    "        \"euros_per_mwh\",\n",
    "        \"data_block_id\",\n",
    "        \"origin_date\",\n",
    "        \"forecast_date\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Rename to reflect hourly timestamps instead of just dates\n",
    "electricity_prices_df = electricity_prices_df.rename(\n",
    "    columns={\n",
    "        \"origin_date\": \"origin_datetime\",\n",
    "        \"forecast_date\": \"forecast_datetime\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ff8ec-d07e-4280-bc9d-11509099e43c",
   "metadata": {},
   "source": [
    "### Partial Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb6464-995d-404d-b663-34b9ce0ea7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check timestamps for duplicates\n",
    "electricity_prices_df.duplicated([\"origin_datetime\", \"forecast_datetime\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ab77a-1546-4408-adaf-2f4893146772",
   "metadata": {},
   "source": [
    "No partial duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0431754-2c7e-4bb2-950e-0c784961306e",
   "metadata": {},
   "source": [
    "### Non-positive Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c2d28-1315-43e2-b010-dcda1ca0d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_prices_df[electricity_prices_df[\"euros_per_mwh\"] <= 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e9d81-64b2-437f-8658-8d8a156a0015",
   "metadata": {},
   "source": [
    "According to the [<u>official comment</u>](https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers/discussion/454932#2523730), such prices are not an error.\n",
    "\n",
    "[<u>Negative pricing</u>](https://en.wikipedia.org/wiki/Negative_pricing) can occur when demand for a product drops or supply increases to an extent that owners or suppliers are prepared to pay others to accept it, in effect setting the price to a negative number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6229fdca-c53e-498a-93fe-6db5a3dcee54",
   "metadata": {},
   "source": [
    "### Consistency and Relationship Validation\n",
    "As with `gas_prices_df`, it is necessary to validate the correlation between `origin_datetime`, `forecast_datetime`, and `data_block_id`.\n",
    "#### Origin and forecast dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61db965b-9986-4363-8c17-c1cf29ae112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that each electricity_origin_date is the next hour compared to\n",
    "# the previous one\n",
    "print(\n",
    "    electricity_prices_df[\"origin_datetime\"]\n",
    "    .diff()\n",
    "    .value_counts(dropna=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ce105-379c-4349-bde1-a6ecaac260e7",
   "metadata": {},
   "source": [
    "There are also values with a 2-hour difference. This might indicate an issue with data collection or other inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4b4597-7d6a-4832-9c4a-261ecf548801",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_prices_df[\n",
    "    electricity_prices_df[\"origin_datetime\"].diff()\n",
    "    == pd.Timedelta(2, \"hour\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee1054-ed4f-4f12-86bc-e6854210912e",
   "metadata": {},
   "source": [
    "A 2-hour time difference occurs at 03:00 on `2022-03-27` and `2023-03-26` due to the transition to daylight saving time. This transition results in an increased time difference between records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc218b-d518-43cc-b1d8-67d6d488a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any forecast dates are not exactly one day after origin dates\n",
    "(\n",
    "    electricity_prices_df[\"origin_datetime\"]\n",
    "    + pd.Timedelta(days=1)\n",
    "    != electricity_prices_df[\"forecast_datetime\"]\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc31320-e82b-42d5-91b7-6ac58f1577b8",
   "metadata": {},
   "source": [
    "- All `origin_datetime` values represent the day before their corresponding `forecast_datetime` values.\n",
    "- The raw data in this dataframe is sorted chronologically by datetime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2a93f-cf52-4dc4-90e5-55708e3a2fb4",
   "metadata": {},
   "source": [
    "#### Origin date and data_block_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69292ea4-4a96-46fd-8999-ed02f8c71fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables to store the differences between the current and\n",
    "# previous row for all rows in origin_datetime and data_block_id\n",
    "electricity_date_diff = electricity_prices_df[\n",
    "    \"origin_datetime\"\n",
    "].dt.date.diff()\n",
    "electricity_id_diff = electricity_prices_df[\"data_block_id\"].diff()\n",
    "\n",
    "# Print unique values to verify that default order reflects sorting\n",
    "# from oldest to newest without skips\n",
    "print(electricity_date_diff.value_counts(dropna=False))\n",
    "print(electricity_id_diff.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb14b95-b710-4962-98d8-878d75db61e0",
   "metadata": {},
   "source": [
    "- `NaT` and `NaN` values correspond to the first row in both columns, where no previous row data is available for comparison.\n",
    "- A value of '0' indicates that the current and previous rows belong to the same day.\n",
    "- A value of '1' indicates the transition from one day to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d1f66-64df-4c89-a7d4-1df37ddae839",
   "metadata": {},
   "outputs": [],
   "source": [
    "for difference in electricity_id_diff.unique():\n",
    "    mismatched_indices_check(\n",
    "        electricity_date_diff, electricity_id_diff, difference\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85706d86-e8f5-40f4-8bab-49a96ad3aa28",
   "metadata": {},
   "source": [
    "- The default order of the raw data for this dataframe is sorted by datetime, and the `data_block_id` reflects the actual data availability, with no errors observed in the `origin_datetime` column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8737f2cc-ed66-4ca8-9f32-695a1f30a724",
   "metadata": {},
   "source": [
    "## forecast_weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e027ae6-fc3e-42e4-89e9-78a947cfc32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_show_info(forecast_weather_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b630472b-23a3-447a-8d9b-7c40cf51c0ac",
   "metadata": {},
   "source": [
    "- Weather forecasts are generated at 2 AM for the next 48 hours. Since the goal is to predict consumption and production for every hour of the following day, only forecasts between +22 and +45 hours_ahead will be used. However, other predictions from the same day can also be utilized as additional predictors.\n",
    "- There are 2 missing values and zero duplicates in the dataframe. Only the `surface_solar_radiation_downwards` column contains missing values. \n",
    "- The `data_block_id` values start from 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9226e72c-48a3-4c88-a410-88f95cc16e87",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd503af-0f8a-4afd-a861-1479bc94843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "forecast_weather_df = forecast_weather_df[\n",
    "    [\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"origin_datetime\",\n",
    "        \"hours_ahead\",\n",
    "        \"forecast_datetime\",\n",
    "        \"data_block_id\",\n",
    "        \"temperature\",\n",
    "        \"dewpoint\",\n",
    "        \"cloudcover_low\",\n",
    "        \"cloudcover_mid\",\n",
    "        \"cloudcover_high\",\n",
    "        \"cloudcover_total\",\n",
    "        \"10_metre_u_wind_component\",\n",
    "        \"10_metre_v_wind_component\",\n",
    "        \"direct_solar_radiation\",\n",
    "        \"surface_solar_radiation_downwards\",\n",
    "        \"snowfall\",\n",
    "        \"total_precipitation\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Change data types to reduce memory usage\n",
    "forecast_weather_df[[\"latitude\", \"longitude\"]] = (\n",
    "    forecast_weather_df[[\"latitude\", \"longitude\"]].round(1).mul(10)\n",
    ")\n",
    "forecast_weather_df[\n",
    "    [\"cloudcover_low\", \"cloudcover_mid\", \"cloudcover_high\", \"cloudcover_total\"]\n",
    "] = (\n",
    "    forecast_weather_df[\n",
    "        [\n",
    "            \"cloudcover_low\",\n",
    "            \"cloudcover_mid\",\n",
    "            \"cloudcover_high\",\n",
    "            \"cloudcover_total\",\n",
    "        ]\n",
    "    ]\n",
    "    .round(2)\n",
    "    .mul(100)\n",
    ")\n",
    "\n",
    "forecast_weather_df = forecast_weather_df.astype(\n",
    "    {\n",
    "        \"latitude\": \"uint16\",\n",
    "        \"longitude\": \"uint16\",\n",
    "        \"origin_datetime\": \"datetime64[ns]\",\n",
    "        \"forecast_datetime\": \"datetime64[ns]\",\n",
    "        \"data_block_id\": \"uint16\",\n",
    "        \"temperature\": \"float32\",\n",
    "        \"dewpoint\": \"float32\",\n",
    "        \"cloudcover_low\": \"uint8\",\n",
    "        \"cloudcover_mid\": \"uint8\",\n",
    "        \"cloudcover_high\": \"uint8\",\n",
    "        \"cloudcover_total\": \"uint8\",\n",
    "        \"10_metre_u_wind_component\": \"float32\",\n",
    "        \"10_metre_v_wind_component\": \"float32\",\n",
    "        \"direct_solar_radiation\": \"float32\",\n",
    "        \"surface_solar_radiation_downwards\": \"float32\",\n",
    "        \"snowfall\": \"float32\",\n",
    "        \"total_precipitation\": \"float32\",\n",
    "    }\n",
    ")\n",
    "forecast_weather_df[\"hours_ahead\"] = pd.to_timedelta(\n",
    "    forecast_weather_df[\"hours_ahead\"], \"h\"\n",
    ")\n",
    "\n",
    "print(f\"Size after transformation: {total_size(forecast_weather_df)} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be752e-5c55-4d29-9932-83f9f04322bc",
   "metadata": {},
   "source": [
    "### Partial Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ce2c3-51ba-4467-b009-9f5841479bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check coordinates and timestamps for duplicates\n",
    "forecast_weather_df.duplicated(\n",
    "    [\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"origin_datetime\",\n",
    "        \"hours_ahead\",\n",
    "        \"forecast_datetime\",\n",
    "    ]\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b599dc6-dac3-4af0-8240-50c55727830b",
   "metadata": {},
   "source": [
    "No partial duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3cf0b7-9b50-47d3-9a4e-ef0874058921",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52033f48-4a41-4348-818b-5c4746715f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_weather_df[forecast_weather_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04cc04f-32d7-4bc6-85a3-33f9e8d8b0f0",
   "metadata": {},
   "source": [
    "- Since there are only two missing values, it could be due to a local issue lasting for just 2 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf762ee-0c72-49b7-8654-56a1fa94b1b5",
   "metadata": {},
   "source": [
    "### Consistency and Relationship Validation\n",
    "Ensure `origin_datetime`, `forecast_datetime`, `hours_ahead` values are coherent within respective `latitude` and `longitude` groups.\n",
    "#### Origin datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c47428-ac46-46d4-a323-59bca3929c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sameness check\n",
    "forecast_geo_gb = forecast_weather_df.groupby([\"latitude\", \"longitude\"])\n",
    "same_groups(forecast_geo_gb, \"origin_datetime\").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921cbdd-020a-4d7c-84f7-326b7bce5151",
   "metadata": {},
   "source": [
    "This dataframe contains 112 unique combinations of `latitude` and `longitude`, representing distinct geographical points for weather forecasts. Only one `False` value corresponds to the first group, indicating that all other location points have identical subsequences of `origin_datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f6ff0f-a89d-40e1-b523-ad6c1dd245e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast creation times\n",
    "forecast_weather_df[\"origin_datetime\"].dt.time.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03160b7-3f66-48fa-bfed-89a964df3799",
   "metadata": {},
   "source": [
    "All forecasts are made at either 1 AM or 2 AM, likely due to shifts associated with transitions to and from daylight saving time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc3f13-51cb-492a-8849-c2201f376e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select latitude and longitude values for the first entry of the\n",
    "# geographical group, as the sequences of origin_datetime values are\n",
    "# identical across each group\n",
    "lat0, lon0 = forecast_weather_df.iloc[0][[\"latitude\", \"longitude\"]]\n",
    "fw_subset_df = forecast_weather_df[\n",
    "    (forecast_weather_df[\"latitude\"] == lat0)\n",
    "    & (forecast_weather_df[\"longitude\"] == lon0)\n",
    "]\n",
    "\n",
    "# Calculate the differences between consecutive origin_datetime values\n",
    "# for the selected geographical group\n",
    "fod_diff = fw_subset_df[\"origin_datetime\"].diff()\n",
    "\n",
    "# Display the counts of unique differences between consecutive\n",
    "# origin_datetime entries\n",
    "fod_diff.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174e10f-fff4-4200-a8c9-8bddc46c2903",
   "metadata": {},
   "source": [
    "- `0 days 00:00:00`: rows corresponding to the same location and day, but with different `hours_ahead` values.\n",
    "- `1 days 00:00:00`: indicates a shift from previous origin day.\n",
    "- `0 days 23:00:00`: likely caused by a transition to daylight saving time.\n",
    "- `1 days 01:00:00`: likely caused by a transition from daylight saving time.\n",
    "- `NaT`: corresponds to the first rows in each group.\n",
    "\n",
    "Based on these differences, it can be concluded that the data in this dataframe is sorted by `origin_datetime` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26318f-f8c2-40d1-a079-a03563369bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where differences in origin_datetime are not 0, 1 day, or\n",
    "# NaT\n",
    "unusual_diff_df = fw_subset_df[\n",
    "    (fod_diff != pd.Timedelta(0))\n",
    "    & (fod_diff != pd.Timedelta(1, \"d\"))\n",
    "    & (~fod_diff.isna())\n",
    "][[\"origin_datetime\"]]\n",
    "\n",
    "# Merge the filtered timestamps with their corresponding time\n",
    "# differences\n",
    "pd.merge(\n",
    "    unusual_diff_df,\n",
    "    fod_diff[unusual_diff_df.index].rename(\"difference\"),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab673bc-b932-42a0-94fd-0fa67c760103",
   "metadata": {},
   "source": [
    "Yes, the autumn shift corresponds to a switch to winter time, and the spring shift corresponds to a switch to summer time. Specifically:\n",
    "- The `origin_datetime` hour is `02:00:00` during the following periods:\n",
    "  - From `2021-09-01` to `2021-10-31`\n",
    "  - From `2022-03-28` to `2022-10-30`\n",
    "  - From `2023-03-27` to `2023-05-30`\n",
    "- The `origin_datetime` hour is `01:00:00` during the following periods:\n",
    "  - From `2021-11-01` to `2022-03-27`\n",
    "  - From `2022-10-31` to `2023-03-26`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac02d1-b632-4aee-93fa-b7901f38b1a8",
   "metadata": {},
   "source": [
    "#### Forecast datetime\n",
    "From the data description:\n",
    "- `forecast_datetime` - The timestamp of the predicted weather. Generated from `origin_datetime` plus `hours_ahead`. This represents the start of the 1-hour period for which weather data are forecasted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d493e884-f1d9-4d25-8339-a7c0083481dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sameness check for other two datetime features\n",
    "for column in (\"forecast_datetime\", \"hours_ahead\"):\n",
    "    print(same_groups(forecast_geo_gb, column).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d59d05-ab69-442d-b4b4-57c770afea97",
   "metadata": {},
   "source": [
    "As with `origin_datetime`, for each geographical group, the sequence of `forecast_datetime` or `hours_ahead` values is identical across the locations. Therefore, it is sufficient to check the `forecast_datetime` and `hours_ahead` features using data from just one location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c40908-f95a-40b3-b80c-3d0871c47474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the equality stated in the dataset description:\n",
    "# origin_datetime + hours_ahead = forecast_datetime\n",
    "unusual_sum_df = fw_subset_df[\n",
    "    fw_subset_df[\"origin_datetime\"]\n",
    "    + fw_subset_df[\"hours_ahead\"]\n",
    "    != fw_subset_df[\"forecast_datetime\"]\n",
    "]\n",
    "unusual_sum_df[\"origin_datetime\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd542c-df3c-4424-b708-ae846d2cb828",
   "metadata": {},
   "source": [
    "Values that do not satisfy the condition occur only within the 48 hours preceding a switch to or from DST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b0b25e-d1cd-4a7f-bc28-9d2a95dcdf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that entries not satisfying the condition will satisfy it after\n",
    "# adding or subtracting respectively.\n",
    "# For autumn,\n",
    "# origin_datetime + hours_ahead == forecast_datetime + 1 hour.\n",
    "# For spring,\n",
    "# origin_datetime + hours_ahead == forecast_datetime - 1 hour.\n",
    "\n",
    "(unusual_sum_df[\"origin_datetime\"]\n",
    " + unusual_sum_df[\"hours_ahead\"]\n",
    " != unusual_sum_df[\"forecast_datetime\"]\n",
    " + pd.to_timedelta(np.where(\n",
    "     unusual_sum_df[\"origin_datetime\"].dt.month == 10,  # October\n",
    "     1,  # Add 1 hour for autumn switch to DST (October)\n",
    "     -1  # Subtract 1 hour for spring switch from DST (March)\n",
    " ), 'h')\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7836cb2-1896-457d-b85a-500b054791fa",
   "metadata": {},
   "source": [
    "All conditions are satisfied after the corresponding addition or subtraction. This means that all previously unsatisfied sums of `origin_datetime` and `hours_ahead` differ from `forecast_datetime` by exactly 1 hour due to the switch to or from DST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc74b68-571d-49bd-b6a5-5a2d748b8904",
   "metadata": {},
   "source": [
    "#### data_block_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e686d6e-4fcd-40c5-8f02-174c2c7b99da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables to store the differences between the current and\n",
    "# previous row for all rows in datetime and data_block_id\n",
    "\n",
    "fw_id_diff = forecast_weather_df[\"data_block_id\"].diff()\n",
    "fw_date_diff = forecast_weather_df[\"origin_datetime\"].dt.date.diff()\n",
    "\n",
    "print(train_id_diff.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edca246-f7fa-47ab-9a70-2977a2cc6012",
   "metadata": {},
   "source": [
    "- A value of '0' indicates that the current and previous rows belong to the same day.\n",
    "- A value of '1' indicates the transition from one day to the next.\n",
    "- `NaN` values correspond to the first row in both columns, where no previous row data is available for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90fa65f-b7b9-41f9-9c3a-9561378ea030",
   "metadata": {},
   "outputs": [],
   "source": [
    "for difference in fw_id_diff.unique():\n",
    "    mismatched_indices_check(fw_date_diff, fw_id_diff, difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0566c0-c418-4c03-9341-8aeed4ee05b5",
   "metadata": {},
   "source": [
    "The default order of the raw data for this dataframe is sorted by datetime, and the `data_block_id` reflects the actual data availability, with no errors observed in the `origin_datetime` column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af9350-9faf-43a3-8ac3-338aaae7f13f",
   "metadata": {},
   "source": [
    "## historical_weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9920a6bd-e99c-4fe1-97d7-62b6c0e66975",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_show_info(historical_weather_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7de862-eb2e-4746-bd1e-3c4d6957f12d",
   "metadata": {},
   "source": [
    "- The `data_block_id` column type is `float64`, unlike in other dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b3b870-e0ed-4ee2-8879-265009971f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if all values in data_block_id are \n",
    "# integers to avoid errors and allow conversion to int type\n",
    "\n",
    "print(np.unique(historical_weather_df.data_block_id.unique() %1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14a2630-f93e-4df8-8c0a-5bcfe7b072f3",
   "metadata": {},
   "source": [
    "No anomalous values; all have a remainder of 0 and can be converted to `uint16` type (which can store values from 0 to 65535), as the minimum value is 1 and the maximum is 637."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661bf2db-f164-49f7-996a-ca11fa3ddda0",
   "metadata": {},
   "source": [
    "- There are no missing values or duplicates in the dataframe.\n",
    "- The `data_block_id` is `float64` because the data in the original file is in \"n.0\" format.\n",
    "- The `data_block_id` values start from 1.\n",
    "- The `cloudcover_...` values are integers from 0 to 100, which differ from those in forecast_weather_df."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea286927-6a59-4770-95c5-7ceee7ab1f4e",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84928470-5fb0-45c6-82f7-6879dd9d0f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "historical_weather_df = historical_weather_df[\n",
    "    [\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"datetime\",\n",
    "        \"data_block_id\",\n",
    "        \"temperature\",\n",
    "        \"dewpoint\",\n",
    "        \"rain\",\n",
    "        \"snowfall\",\n",
    "        \"surface_pressure\",\n",
    "        \"cloudcover_total\",\n",
    "        \"cloudcover_low\",\n",
    "        \"cloudcover_mid\",\n",
    "        \"cloudcover_high\",\n",
    "        \"windspeed_10m\",\n",
    "        \"winddirection_10m\",\n",
    "        \"shortwave_radiation\",\n",
    "        \"direct_solar_radiation\",\n",
    "        \"diffuse_radiation\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Change data types to reduce memory usage\n",
    "historical_weather_df[[\"latitude\", \"longitude\"]] = (\n",
    "    historical_weather_df[[\"latitude\", \"longitude\"]].round(1).mul(10)\n",
    ")\n",
    "historical_weather_df = historical_weather_df.astype(\n",
    "    {\n",
    "        \"latitude\": \"uint16\",\n",
    "        \"longitude\": \"uint16\",\n",
    "        \"datetime\": \"datetime64[ns]\",\n",
    "        \"data_block_id\": \"uint16\",\n",
    "        \"temperature\": \"float32\",\n",
    "        \"dewpoint\": \"float32\",\n",
    "        \"rain\": \"float32\",\n",
    "        \"snowfall\": \"float32\",\n",
    "        \"surface_pressure\": \"float32\",\n",
    "        \"cloudcover_total\": \"uint8\",\n",
    "        \"cloudcover_low\": \"uint8\",\n",
    "        \"cloudcover_mid\": \"uint8\",\n",
    "        \"cloudcover_high\": \"uint8\",\n",
    "        \"windspeed_10m\": \"float32\",\n",
    "        \"winddirection_10m\": \"uint16\",\n",
    "        \"shortwave_radiation\": \"uint16\",\n",
    "        \"direct_solar_radiation\": \"uint16\",\n",
    "        \"diffuse_radiation\": \"uint16\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Shift the timestamp by 24 hours so that it reflects the previous\n",
    "# day's weather aligning it with the current day for merging.\n",
    "# historical_weather_df[\"datetime\"] = historical_weather_df[\n",
    "#     \"datetime\"\n",
    "# ] + pd.Timedelta(\"24 h\")\n",
    "\n",
    "print(f\"Size after transformation: {total_size(historical_weather_df)} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d013f4-5623-48c6-b922-296a58a18ee7",
   "metadata": {},
   "source": [
    "### Partial Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa1a712-0acd-43d5-acaf-64ad4eb42d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check coordinates and timestamps for duplicates\n",
    "historical_weather_df.duplicated([\"latitude\", \"longitude\", \"datetime\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505230f9-9d63-4b99-b2f4-7f024e536215",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_dups = historical_weather_df[\n",
    "    historical_weather_df.duplicated(\n",
    "        [\"latitude\", \"longitude\", \"datetime\"], keep=False\n",
    "    )\n",
    "]\n",
    "hw_dups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe205a4-c91b-4647-b2be-d1994b22e25e",
   "metadata": {},
   "source": [
    "In both duplicate pairs, one entry contains more zeros than the other. Duplicates can be processed by selecting one entry from each pair or creating an entry with mean values. It is helpful to check other nearby entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743efe89-776a-4f01-8565-f529976c7d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_weather_df.loc[\n",
    "    (\n",
    "        historical_weather_df[\"latitude\"]\n",
    "        == historical_weather_df.iloc[hw_dups.index[0]].latitude\n",
    "    )\n",
    "    & (\n",
    "        historical_weather_df[\"longitude\"]\n",
    "        == historical_weather_df.iloc[hw_dups.index[0]].longitude\n",
    "    )\n",
    "    & (\n",
    "        historical_weather_df[\"datetime\"].isin(\n",
    "            pd.date_range(\n",
    "                start=historical_weather_df.iloc[hw_dups.index[0]].datetime\n",
    "                - pd.Timedelta(\"2 h\"),\n",
    "                periods=5,\n",
    "                freq=\"h\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3daf23-52c8-42a6-b579-04eddd116d4a",
   "metadata": {},
   "source": [
    "In this case, entry `1176339` looks less probable than entry `1176340`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9c52e-11ff-4657-977d-b267080e5999",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_to_drop = [1176339]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c99b510-56ff-417a-a29d-581ac1dad5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_weather_df.loc[\n",
    "    (\n",
    "        historical_weather_df[\"latitude\"]\n",
    "        == historical_weather_df.iloc[hw_dups.index[2]].latitude\n",
    "    )\n",
    "    & (\n",
    "        historical_weather_df[\"longitude\"]\n",
    "        == historical_weather_df.iloc[hw_dups.index[2]].longitude\n",
    "    )\n",
    "    & (\n",
    "        historical_weather_df[\"datetime\"].isin(\n",
    "            pd.date_range(\n",
    "                start=historical_weather_df.iloc[hw_dups.index[2]].datetime\n",
    "                - pd.Timedelta(\"2 h\"),\n",
    "                periods=5,\n",
    "                freq=\"h\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2373b7-fab8-462e-893b-bb2b635a87a7",
   "metadata": {},
   "source": [
    "In this case, entry `1176343` looks less probable than entry `1176342`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1446d3c2-37ce-4532-b3fb-c95974e1a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_to_drop.append(1176343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693e15f4-701b-4ab0-9536-97a02f69fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_weather_df = historical_weather_df.drop(\n",
    "    index=hw_to_drop\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b34275-39c0-4fb0-9bb6-8dfaadc9f14b",
   "metadata": {},
   "source": [
    "### Consistency and Relationship Validation\n",
    "Ensure `datetime` values are coherent within respective `latitude` and `longitude` groups.\n",
    "#### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1d1f5-71bd-4608-badb-3a153dc49e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sameness check\n",
    "historical_geo_gb = historical_weather_df.groupby([\"latitude\", \"longitude\"])\n",
    "sg_hw = same_groups(historical_geo_gb, \"datetime\")\n",
    "sg_hw.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291080aa-d3ba-46b4-88bc-d5b1fb425d04",
   "metadata": {},
   "source": [
    "Unlike in `forecast_weather_df`, this dataframe contains 2 unique `datetime` sequences.\n",
    "This dataframe contains 112 unique combinations of `latitude` and `longitude` (after dropping two entries), representing distinct geographical points for historical weather data. Only one `False` value corresponds to the first group, indicating that all other location points have identical subsequences of `datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b88cd-6d5e-4a0f-b3ac-2a07ac4baf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_sample = historical_weather_df[\n",
    "    (historical_weather_df[\"latitude\"] == lat0)\n",
    "    & (historical_weather_df[\"longitude\"] == lon0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340339b-684e-4574-8ade-6a6cc7e1e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the counts of unique differences between consecutive datetime\n",
    "# values for one geographical pount.\n",
    "h_sample[\"datetime\"].diff().value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fb95f5-0a7c-4ce0-8275-b5d8c19ac0ad",
   "metadata": {},
   "source": [
    "- `NaT`: corresponds to the first row.\n",
    "- `0 days 01:00:00`: indicates the transition from one hour to the next.\n",
    "\n",
    "Based on these differences, it can be concluded that the data in this dataframe is sorted by `datetime` by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b3ee6-b744-4729-bacf-f6e77c319d93",
   "metadata": {},
   "source": [
    "#### data_block_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edef438-9017-4802-ab0b-ea8fd9bfba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_weather_df['data_block_id'].diff().value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8495cdb8-8bdb-426c-b48d-71c94a695ee4",
   "metadata": {},
   "source": [
    "All `data_block_id` values also increase incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad68b240-f20f-4807-a8a2-6791e88d07e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all data_block_id entries increment at 11 AM as stated in\n",
    "# the dataset description\n",
    "h_sample.loc[\n",
    "    h_sample[\"data_block_id\"].diff() == 1, \"datetime\"\n",
    "].dt.time.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c344f-8138-4ee5-88a3-9760bd791bf6",
   "metadata": {},
   "source": [
    "## county_id_to_name_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d68c097-0470-4f60-a6a1-a0ec3c01b3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_id_to_name_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e93eb-220c-45d3-9e4e-c227aa3f19f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_id_to_name_map = county_id_to_name_map.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d464b8-4691-4b01-a232-ac102d8a0fd1",
   "metadata": {},
   "source": [
    "The value `unknown` can be interpreted in multiple ways. Firstly, it may represent missing values. Secondly, it could indicate a business that spans a very large area and is physically located in multiple counties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f1a11-c115-44ad-b10c-b768d0ed4c28",
   "metadata": {},
   "source": [
    "## station_county_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aad6d7-9159-4978-a1b6-b4c06744319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_show_info(station_county_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9eb7cc-485e-4ab3-9d4e-53001f196d48",
   "metadata": {},
   "source": [
    "- More than half of the values are missing in two columns. This is likely because the dataframe covers the outer areas around Estonia.\n",
    "- There are 15 unique `county` values plus a `NaN` value, whereas the other two dataframes that contain county data have 16 unique values without any `NaN`. This is probably because `unknown` might not have been used as a placeholder for missing data in this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86dfa2b-39d4-44b7-b896-cc8efbc28a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    station_county_mapping[\"county\"].drop_duplicates().sort_values().tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418fa6d-ce68-479b-8e43-29141f23e797",
   "metadata": {},
   "source": [
    "Value `12` is not present in this list. To visualize all locations on the map, `NaN` values could be filled with `12`, since it might represent an `unknown` value from other dataframes. However, these locations might be outside Estonia and may not correspond to an `unknown` county in the other dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3751f18-1c92-4bf5-a4ed-94c07ccab992",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd439aa-1337-4ff5-a4d5-af8eca50b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "station_county_mapping = station_county_mapping[\n",
    "    [\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"county_name\",\n",
    "        \"county\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "station_county_mapping[[\"latitude\", \"longitude\"]] = (\n",
    "    station_county_mapping[[\"latitude\", \"longitude\"]].round(1).mul(10)\n",
    ")\n",
    "\n",
    "station_county_mapping[[\"county_name\", \"county\"]] = station_county_mapping[\n",
    "    [\"county_name\", \"county\"]\n",
    "].fillna({\"county_name\": \"unknown\", \"county\": 12})\n",
    "\n",
    "station_county_mapping[\"county_name\"] = station_county_mapping[\n",
    "    \"county_name\"\n",
    "].str.lower()\n",
    "\n",
    "station_county_mapping = (\n",
    "    station_county_mapping.astype(\n",
    "        {\n",
    "            \"latitude\": \"uint16\",\n",
    "            \"longitude\": \"uint16\",\n",
    "            \"county_name\": \"category\",\n",
    "            \"county\": \"uint8\",\n",
    "        }\n",
    "    )\n",
    "    .astype({\"county\": \"category\"})\n",
    "    .sort_values([\"latitude\", \"longitude\"], ignore_index=True)\n",
    ")\n",
    "\n",
    "station_county_mapping = station_county_mapping.rename(\n",
    "    columns={\n",
    "        \"county_name\": \"county\",\n",
    "        \"county\": \"county_index\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaaba20-3fca-4758-bb22-d9f4bc714826",
   "metadata": {},
   "source": [
    "# 4. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94814a49-52ab-4f83-8a4d-e0bb6287adab",
   "metadata": {},
   "source": [
    "## Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65064c4-3ecc-4931-a09c-0cde335632d3",
   "metadata": {},
   "source": [
    "Verify that all unique locations in `forecast_weather_df`, `historical_weather_df`, and `station_county_mapping` match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3879ad3-a16f-42a3-ae69-17e9734bb2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    unique_combinations(forecast_weather_df, [\"latitude\", \"longitude\"])\n",
    "    != unique_combinations(historical_weather_df, [\"latitude\", \"longitude\"])\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b6f7b6-abbd-404d-bcdc-eda3bdbafed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    unique_combinations(forecast_weather_df, [\"latitude\", \"longitude\"])\n",
    "    != station_county_mapping[[\"latitude\", \"longitude\"]]\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8532828b-2b1b-451a-a5a4-91a4196c0d9a",
   "metadata": {},
   "source": [
    "Coordinates of all points from the three dataframes are identical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa1e50-c28e-4719-8d32-08cbea1818ca",
   "metadata": {},
   "source": [
    "#### Visualize all unique location points on an Estonia map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed87118-fb49-4e44-ad67-e7077f4ac904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract county names and their centroid coordinates for labeling\n",
    "region_names, centroid_lats, centroid_lons = [], [], []\n",
    "\n",
    "for feature in estonia_geojson[\"features\"]:\n",
    "    region_name = feature[\"properties\"].get(\"name\")\n",
    "    lat = round(shape(feature[\"geometry\"]).centroid.y, 2)\n",
    "    lon = round(shape(feature[\"geometry\"]).centroid.x, 2)\n",
    "\n",
    "    region_names.append(region_name)\n",
    "    centroid_lats.append(lat)\n",
    "    centroid_lons.append(lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362775ed-800e-4010-b483-7a3e91e8c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_map(\n",
    "    lat=station_county_mapping.sort_values(\"county_index\")[\"latitude\"] / 10,\n",
    "    lon=station_county_mapping.sort_values(\"county_index\")[\"longitude\"] / 10,\n",
    "    color=station_county_mapping.sort_values(\"county_index\")[\"county\"],\n",
    "    zoom=6.5,\n",
    "    width=1360,\n",
    "    height=820,\n",
    "    color_discrete_sequence=PALETTE,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    map=dict(\n",
    "        layers=[\n",
    "            dict(\n",
    "                sourcetype=\"geojson\",\n",
    "                source=estonia_geojson,\n",
    "                type=\"line\",\n",
    "                color=\"black\",\n",
    "                line=dict(width=0.5),\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scattermap(\n",
    "        lat=centroid_lats,\n",
    "        lon=centroid_lons,\n",
    "        mode=\"text\",\n",
    "        text=region_names,\n",
    "        textfont=dict(size=12, color=\"black\"),\n",
    "    )\n",
    ")\n",
    "fig.update_traces(marker=dict(size=16))\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "fig.write_image(\"map.png\")\n",
    "Image(\"map.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda6984-3301-4f97-a173-0701434e73d6",
   "metadata": {},
   "source": [
    "## DataFrames Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef292b-6fdc-48f8-89e9-e110da1e1404",
   "metadata": {},
   "source": [
    "For the `unknown` county in the target, both forecast and historical weather data will be aggregated from all counties, but data from `station_county_mapping` with `12` (`NaN`) values for the county (corresponding to geopoints in water or outside Estonia's boundaries) will be excluded. While the option of adding border or water points for the counties can be considered, such combinations will not be used at the moment.\n",
    "\n",
    "Additionally, an extra flag feature indicating daylight saving time is necessary, as it influences various factors, such as the start of the workday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc327694-91c0-4938-a916-73c4066a365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_weather_data(df: pd.DataFrame, mapper: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute mean aggregated weather data per county and overall.\n",
    "\n",
    "    This function creates a deep copy of the input DataFrame, merges it\n",
    "    with the county mapping, and computes the mean of all numerical\n",
    "    weather features grouped by time-related columns and county, as\n",
    "    well as the overall mean across all counties.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input weather data with latitude, longitude, datetime,\n",
    "        numerical weather features.\n",
    "    mapper : pd.DataFrame\n",
    "        DataFrame that maps each location to a county containing\n",
    "        \"county\", \"latitude\", and \"longitude\" columns. Only rows with a\n",
    "        known \"county\" (i.e., not 'unknown') are used for merging.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with mean aggregated weather data per county and overall.\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge weather data with county mapping, excluding 'unknown'\n",
    "    # values which correspond to locations outside of Estonia\n",
    "    df = df.copy()\n",
    "    df = pd.merge(\n",
    "        left=mapper.loc[\n",
    "            mapper[\"county\"] != \"unknown\",\n",
    "            [\"county\", \"latitude\", \"longitude\"],\n",
    "        ],\n",
    "        right=df,\n",
    "        how=\"left\",\n",
    "        on=[\"latitude\", \"longitude\"],\n",
    "        validate=\"1:m\",\n",
    "    )\n",
    "\n",
    "    # Identify and exclude grouping columns from average calculation\n",
    "    groups = [\n",
    "        c\n",
    "        for c in df.columns.tolist()\n",
    "        if \"datetime\" in c or c == \"hours_ahead\" or c == \"data_block_id\"\n",
    "    ]\n",
    "    excluded_c = groups + [\"county\", \"latitude\", \"longitude\",]\n",
    "    avg_c = [c for c in df.columns if c not in excluded_c]\n",
    "    dtypes = df[avg_c].dtypes.to_dict()\n",
    "    to_round = [k for k, v in dtypes.items() if np.issubdtype(v, np.integer)]\n",
    "\n",
    "    # Compute overall mean (county='unknown'), then per-county mean,\n",
    "    # and concatenate both results\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            df.groupby(groups, as_index=False, observed=True)[avg_c]\n",
    "            .mean()\n",
    "            .assign(county=\"unknown\"),\n",
    "            df.groupby(groups + [\"county\"], as_index=False, observed=True)[\n",
    "                avg_c\n",
    "            ].mean(),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    dtypes.update({\"county\": \"category\"})\n",
    "\n",
    "    # Round integer-derived columns back to integers and cast all\n",
    "    # columns to original dtypes\n",
    "    df[to_round] = df[to_round].round()\n",
    "    df = df.astype(dtypes)\n",
    "    return df[[\"county\"] + groups + avg_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ec66c-a72a-4998-b58e-fb7cdf742c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lag(df: pd.DataFrame, dt: str, lag: int, c: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Shift 'dt' column by 'lag' days and rename the 'c' column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame.\n",
    "    dt : str\n",
    "        Name of the datetime column to be shifted.\n",
    "    lag : int\n",
    "        Number of days to shift.\n",
    "    c : str\n",
    "        Name of the column to rename.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with the shifted datetime column and renamed target\n",
    "        column.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If 'lag' is less than 2.\n",
    "    \"\"\"\n",
    "    if lag < 2:\n",
    "        raise ValueError(f\"'lag' must be at least 2 days, got {lag}\")\n",
    "    return df.assign(**{dt: df[dt] + pd.Timedelta(days=lag)}).rename(\n",
    "        columns={c: f\"{lag}d_lag_{c}\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ca38ea-3722-4682-b495-a8b72f9a3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_average(\n",
    "    dfgb: DataFrameGroupBy,\n",
    "    c: str,\n",
    "    window: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute rolling mean for a specified column of a grouped DataFrame\n",
    "    and shift the datetime column by 48Â hours.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfgb : DataFrameGroupBy\n",
    "        Grouped DataFrame (result of df.groupby(..., as_index=False)),\n",
    "        where the original DataFrame was sorted by the datetime index.\n",
    "    c : str\n",
    "        Name of the column to aggregate.\n",
    "    window : int\n",
    "        Rolling window size in hours (min_periods=window).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing:\n",
    "        - all grouping columns,\n",
    "        - the datetime column, shifted by 48Â h,\n",
    "        - a new column with the rolling mean.\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        dfgb[c]\n",
    "        .rolling(pd.Timedelta(f\"{window}h\"), min_periods=window, closed=\"left\")\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .pipe(\n",
    "            lambda d: d.assign(\n",
    "                **{d.columns[0]: d[d.columns[0]] + pd.Timedelta(hours=48)}\n",
    "            )\n",
    "        )\n",
    "        .rename(columns={c: f\"{window}h_ma_2d_lag_{c}\"})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4d228-5fa8-40dd-bc0a-9d3612eae7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfgb = df.set_index(\"datetime\").sort_index().groupby(CATEGORICAL_C, as_index=False, observed=False)\n",
    "# [\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09901e95-e569-488c-a769-8a94e701265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_moving_average(dfgb, 'target', 2).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf385bd-4653-43fb-8a66-9830dbb58947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop spring NaNs and impute autumn NaNs with interpolated values\n",
    "df = train_df.loc[~train_df[\"datetime\"].isin(na_datetimes[1::2])].assign(\n",
    "    target=lambda x: x[\"target\"].interpolate()\n",
    ")\n",
    "\n",
    "# # Add a flag indicating Daylight Saving Time\n",
    "# df[\"dst\"] = ~(\n",
    "#     ((df.datetime >= na_datetimes[0]) & (df.datetime < na_datetimes[1]))\n",
    "#     | ((df.datetime >= na_datetimes[2]) & (df.datetime < na_datetimes[3]))\n",
    "# )\n",
    "\n",
    "# estonia_holidays = holidays.EE(years=range(2021, 2024), language='en_US')\n",
    "# for date, name in estonia_holidays.items():\n",
    "#     print(date, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b127d82-5d91-44ef-b160-2ae78f9f0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # target lag features \n",
    "# for lag in [2, 3, 7]:\n",
    "#     df = df.merge(\n",
    "#         add_lag(df[TARGET_C + ['target']], \"datetime\", lag, 'target'),\n",
    "#         how=\"left\",\n",
    "#         on=TARGET_C,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd500b-e9b9-47ab-bccf-da228e4220dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rolling mean std 48, rolling_mean_96, roll_min_48, roll_max_48\n",
    "# # delta_48, pct_change_48\n",
    "# # rolling_mean_temp_48, temp_forecast_horizon\n",
    "\n",
    "# for r in [24, 24 * 2, 24 * 7]:\n",
    "#     display(df.set_index(\"datetime\").sort_index().groupby(CATEGORICAL_C, as_index=False, observed=False)[\"target\"].rolling(pd.Timedelta(f\"{r}h\"), min_periods=r, closed=\"left\").mean().iloc[20:30].assign(new=lambda x: x.index + pd.Timedelta(\"48 h\")).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221d13b-f769-4251-8aab-a714748e039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[\n",
    "#     (df[\"county\"] == \"harjumaa\")\n",
    "#     & (df[\"product_type\"] == \"combined\")\n",
    "#     & (df[\"is_business\"] == \"business\")\n",
    "#     & (df[\"is_consumption\"] == \"consumption\")\n",
    "# ].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd118632-20d1-4c28-bd01-8313decdbc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (q[CATEGORICAL_C] == q.iloc[0][CATEGORICAL_C]).all(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77482d8c-297a-40ac-93ba-f2f4ceadfbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"winter_time\"] = np.where(\n",
    "#     df.dst, df.datetime - pd.Timedelta(\"1 h\"), df.datetime\n",
    "# )\n",
    "# for d in na_datetimes:\n",
    "#     display(\n",
    "#         df.loc[\n",
    "#             ((df[CATEGORICAL_C] == df.iloc[0][CATEGORICAL_C]).all(axis=1))\n",
    "#             & (\n",
    "#                 df[\"datetime\"].isin(\n",
    "#                     pd.date_range(\n",
    "#                         d - pd.Timedelta(\"1 h\"),\n",
    "#                         d + pd.Timedelta(\"1 h\"),\n",
    "#                         freq=\"h\",\n",
    "#                     )\n",
    "#                 )\n",
    "#             ),\n",
    "#             [\"datetime\", \"winter_time\", \"dst\", \"target\"],\n",
    "#         ]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e3483-13d3-4a35-a746-76b3ed48c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concat df without duplicated hours (spring) with df with added skipped time (autumn)\n",
    "# test_df = (\n",
    "#     pd.concat(\n",
    "#         [\n",
    "#             df.loc[~df[\"datetime\"].isin(na_datetimes[1::2])],\n",
    "#             df.loc[df[\"datetime\"].isin(na_datetimes[::2])].assign(\n",
    "#                 winter_time=lambda x: x[\"winter_time\"] - pd.Timedelta(\" 1h \")\n",
    "#             ),\n",
    "#         ]\n",
    "#     )\n",
    "#     .sort_values(\n",
    "#         [\n",
    "#             \"county\",\n",
    "#             \"product_type\",\n",
    "#             \"is_business\",\n",
    "#             \"is_consumption\",\n",
    "#             \"winter_time\",\n",
    "#         ]\n",
    "#     )\n",
    "#     .reset_index(drop=True)\n",
    "#     .assign(target=lambda x: x[\"target\"].interpolate())\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99620cb9-008f-4735-8a06-2a7650f8a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    left=df.drop(columns=[\"prediction_unit_id\"]),\n",
    "    right=client_df.drop(columns=[\"date\"]),\n",
    "    how='left',\n",
    "    on=[\"county\", \"product_type\", \"is_business\", \"data_block_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb906d2-cca6-4613-99c8-b9b0f4936596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(\n",
    "    right=gas_prices_df.drop(columns=[\"origin_date\", \"forecast_date\"]),\n",
    "    on=[\"data_block_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edae084-0f83-4a76-ba4c-474dbec29336",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(\n",
    "    right=electricity_prices_df.drop(\n",
    "        columns=[\"origin_datetime\", \"forecast_datetime\"]\n",
    "    ),\n",
    "    left_on=[\"datetime\", \"data_block_id\"],\n",
    "    right_on=[\"electricity_datetime\", \"data_block_id\"],\n",
    ").drop(columns=[\"electricity_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558a3fde-9e05-4505-9c86-b0cc93e86ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"f1_\"  # 1 Prefix for columns related to the 1 day forecast\n",
    "df = df.merge(\n",
    "    avg_weather_data(forecast_weather_df, station_county_mapping).add_prefix(\n",
    "        fp\n",
    "    ),\n",
    "    how=\"left\",\n",
    "    left_on=[\"county\", \"datetime\", \"data_block_id\"],\n",
    "    right_on=[\n",
    "        fp + c for c in [\"county\", \"forecast_datetime\", \"data_block_id\"]\n",
    "    ],\n",
    ").drop(\n",
    "    columns=[\n",
    "        fp + c\n",
    "        for c in [\n",
    "            \"county\",\n",
    "            \"origin_datetime\",\n",
    "            \"hours_ahead\",\n",
    "            \"forecast_datetime\",\n",
    "            \"data_block_id\",\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f30bfe-fd51-4910-b075-f78a393d5a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 2â€‘day lag of historical data since data for the previous day is\n",
    "# not fully available (data_block_id increments at 11:00)\n",
    "hp = \"h2_\"  # Prefix for columns related to 2 day historical data\n",
    "hw_df = avg_weather_data(historical_weather_df, station_county_mapping)\n",
    "hw_df[\"fully_available_at\"] = hw_df[\"datetime\"] + pd.Timedelta(\"2 d\")\n",
    "df = df.merge(\n",
    "    hw_df.add_prefix(hp),\n",
    "    how=\"left\",\n",
    "    left_on=[\"county\", \"datetime\"],\n",
    "    right_on=[hp + c for c in [\"county\", \"fully_available_at\"]],\n",
    ").drop(\n",
    "    columns=[\n",
    "        hp + c\n",
    "        for c in [\n",
    "            \"county\",\n",
    "            \"datetime\",\n",
    "            \"fully_available_at\",\n",
    "            \"data_block_id\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "del hw_df  # Remove temporal groupping df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ce18a-5cf7-4d5a-82e3-2b7fb3970f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1f0c2-cf9b-41f1-b7e0-8fbed13f5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "sns.heatmap(\n",
    "    df.drop(\n",
    "        columns=[\n",
    "            \"county\",\n",
    "            \"product_type\",\n",
    "            \"is_business\",\n",
    "            \"is_consumption\",\n",
    "            \"datetime\",\n",
    "            \"data_block_id\"\n",
    "        ]\n",
    "    ).corr(),\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    annot_kws={\"size\": 8}\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c546096e-65ae-4472-b213-9c824b70e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add historical weather data feateurs\n",
    "# Add sun/radiation/angle feature or combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac9455c-685b-4905-8d07-e1ba8276a1f8",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4f9473-da93-4038-823b-1ea8530cb979",
   "metadata": {},
   "source": [
    "### Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779643df-d453-4315-bbee-0112c335787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating target's discrete intervals\n",
    "bins = 10\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "target_bins_percentage = (\n",
    "    pd.cut(df.target, bins=bins, precision=0)\n",
    "    .value_counts()\n",
    "    .div(len(df))\n",
    "    .mul(100)\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "# Due to rounding, the sum of percentages is not exactly 100.0, so\n",
    "# adjust the first value to ensure the total sum is 100.0\n",
    "target_bins_percentage[0] += 100 - target_bins_percentage.sum()\n",
    "target_bins_percentage = [f\"{i:.2f}%\" for i in target_bins_percentage]\n",
    "\n",
    "target_max = df.target.max()\n",
    "ticks = range(0, int(target_max) + 1, int(target_max / bins))\n",
    "\n",
    "ax = sns.histplot(\n",
    "    data=df.target,\n",
    "    bins=bins,\n",
    "    kde=True,\n",
    "    linewidth=0.5,\n",
    ")\n",
    "\n",
    "# Adding group percentage to the top of each bar\n",
    "ax.bar_label(ax.containers[0], target_bins_percentage, padding=6, fontsize=11)\n",
    "\n",
    "plt.xticks(ticks=ticks, rotation=0)\n",
    "\n",
    "# Using a logarithmic scale for the y-axis for better visualization\n",
    "# of small quantities of target values\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.title(\n",
    "    f\"Histogram of {bins} discrete bins of the target values with KDE-line\",\n",
    "    fontsize=13,\n",
    ")\n",
    "plt.xlabel(\"Target values\", fontsize=10)\n",
    "plt.ylabel(\"Count, log scale\", fontsize=10, rotation=0, labelpad=45)\n",
    "plt.grid(False, axis=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b5ac5-fe6d-42e8-bbbe-4912c562dacc",
   "metadata": {},
   "source": [
    "- The target distribution is non-normal and right-skewed, likely because many prosumers are individuals with contracts for their homes, resulting in lower consumption/generation values.\n",
    "- The first discrete bin contains more values than all the other bins combined, with the KDE line confirming that most values are concentrated below 500 (which corresponds to roughly the left third of the range of the first bin on the x-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c1886-a9e8-46dd-879d-94380d0df6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = 3  # The number of boxenplot levels\n",
    "\n",
    "# List with data for additional lines and text started from .25 because\n",
    "# lower corresponding percentiles are the same\n",
    "\n",
    "levels_list = [0.25] + np.cumsum(\n",
    "    [0.5 / pow(2, i) for i in range(levels + 1)]\n",
    ").tolist()\n",
    "levels_values = df.target.describe(levels_list)[3:]\n",
    "\n",
    "plt.figure(figsize=(3, 7.5))\n",
    "ax = sns.boxenplot(\n",
    "    df,\n",
    "    y=\"target\",\n",
    "    linewidth=0,\n",
    "    k_depth=levels,\n",
    "    flier_kws={\n",
    "        \"marker\": \".\",\n",
    "        \"s\": 0.1,\n",
    "    },\n",
    ")\n",
    "\n",
    "ax.set_xlim(ax.get_xlim()[0], ax.get_xlim()[1])\n",
    "\n",
    "plt.hlines(levels_values.values, 0, ax.get_xlim()[1], \"tab:orange\", lw=1)\n",
    "\n",
    "for label, value in levels_values.items():\n",
    "    plt.text(\n",
    "        ax.get_xlim()[1] + 0.03,\n",
    "        value,\n",
    "        f\"{label}: {value:.2f}\",\n",
    "        fontsize=11,\n",
    "        ha=\"left\",\n",
    "        va=\"center\",\n",
    "    )\n",
    "\n",
    "ax.set_ylim(-0.1, 30_000)\n",
    "plt.yscale(\"symlog\", linthresh=1)\n",
    "plt.title(f\"Boxenplot of the target values with {levels} levels\", fontsize=13)\n",
    "plt.ylabel(\"Target values, log scale\", fontsize=10, rotation=0, labelpad=65)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df2c8d5-4405-47b7-b54f-92943f139cc0",
   "metadata": {},
   "source": [
    "- Q<sub>1</sub> â‰ˆ 0.38\n",
    "- Q<sub>2</sub> â‰ˆ 31.26\n",
    "- Q<sub>3</sub> â‰ˆ 180.63\n",
    "\n",
    "There are only two levels on the Q<sub>1</sub> side of the boxenplot (as opposed to three levels on the Q<sub>3</sub> side), which means that there is a huge number of identical values that cannot be separated. That is, two different percentiles (6.25% and 12.5%) have the same value, which equal to the minimum value - 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271b8e51-1fbd-469e-8c2b-8b58785bba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e0b346-07d0-4a33-aa46-6a7a3f7b035d",
   "metadata": {},
   "source": [
    "Zero values are the most common and occur in more than 15% of the cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe34465-1880-4666-bcef-c29f0bf9f5d4",
   "metadata": {},
   "source": [
    "### Target Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0039849a-9a59-4fa5-a222-d3701273e83a",
   "metadata": {},
   "source": [
    "Both classes have equal numbers of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea5416-7d14-47f0-808b-82c29bf29dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consumption values are multiplied by (-1) for better visualisation\n",
    "df[\"modified_target\"] = np.where(\n",
    "    df[\"is_consumption\"] == \"consumption\",\n",
    "    df[\"target\"].mul(-1),\n",
    "    df[\"target\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c78a3d-03ee-4f5f-8a6d-eebb94736a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new time-related features based on datetime\n",
    "\n",
    "# df[\"hour\"] = df[\"datetime\"].dt.hour.astype(\"uint8\")\n",
    "# df[\"day_of_week\"] = df[\"datetime\"].dt.day_of_week.astype(\"uint8\")\n",
    "# df[\"day\"] = df[\"datetime\"].dt.day.astype(\"uint16\")\n",
    "\n",
    "# df[\"month\"] = df[\"datetime\"].dt.month.astype(\"int8\")\n",
    "# df[\"date\"] = df[\"datetime\"].dt.date.astype(\"category\")\n",
    "# df[\"quarter\"] = df[\"datetime\"].dt.quarter.astype(\"int8\")\n",
    "# df[\"year\"] = df[\"datetime\"].dt.year.astype(\"uint16\")\n",
    "\n",
    "df[\"day\"] = df[\"datetime\"].dt.day.astype(\"uint8\").astype(\"category\")\n",
    "df[\"month\"] = df[\"datetime\"].dt.month.astype(\"uint8\").astype(\"category\")\n",
    "df[\"year\"] = df[\"datetime\"].dt.year.astype(\"uint16\").astype(\"category\")\n",
    "df[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]])\n",
    "\n",
    "# df[\"year_week\"] = df['datetime'].dt.strftime('%Y-%W').astype(\"category\")\n",
    "# df[\"year_month\"] = df['datetime'].dt.strftime('%Y-%m').astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f46883-d283-46f4-8c0c-27d0b5500ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 8))\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x=\"datetime\",\n",
    "    y=\"modified_target\",\n",
    "    hue=\"is_consumption\",\n",
    "    s=2,\n",
    ")\n",
    "\n",
    "target_handles = [\n",
    "    circle_label(\"tab:blue\", \"Consumption\"),\n",
    "    circle_label(\"tab:orange\", \"Production\"),\n",
    "]\n",
    "plt.legend(\n",
    "    handles=target_handles,\n",
    "    title=\"Target type\",\n",
    "    title_fontsize=12,\n",
    "    bbox_to_anchor=(1, 1),\n",
    "    loc=\"upper left\",\n",
    "    borderaxespad=0,\n",
    "    frameon=False,\n",
    "    fontsize=11,\n",
    ")\n",
    "\n",
    "ax.xaxis.set_major_locator(MonthLocator(interval=2))\n",
    "ax.yaxis.set_major_locator(MultipleLocator(2000))\n",
    "\n",
    "plt.title(\"Target values over time\", fontsize=13)\n",
    "plt.xlabel(\"Date\", fontsize=10)\n",
    "plt.ylabel(\"Target\", fontsize=10, rotation=0, labelpad=30)\n",
    "plt.grid(color=\"grey\", linewidth=0.5, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae56b4b-3871-40b2-9ea1-119ebb5f71ac",
   "metadata": {},
   "source": [
    "Notable observations and assumptions:\n",
    "\n",
    "- Electricity consumption and production values have been increasing year over year.\n",
    "- The plot shows a wider range of values for consumption, likely because many production values are 0 or near 0. As a result, target consumption values tend to be higher in terms of descriptive statistics.\n",
    "- Seasonal cycles: Production values approach zero during winter and increase toward midsummer, while consumption behaves oppositely, peaking in winter and decreasing in summer.\n",
    "- Weekly cycles: Both consumption and production target values exhibit cyclic patterns, likely tied to working days and weekends.\n",
    "- A decrease in maximum values and the presence of 'voids' for high consumption values are observed around the New Year holidays, which could suggest a pause in business operations during this period (i.e., target values with `is_business` = True).\n",
    "\n",
    "Given the high point density near 0 and the prevalence of high values, creating a symlog variant of the scatterplot with values grouped by year-week would be beneficial. Furthermore, using color differentiation by `product_type` instead of `is_consumption` would enhance the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8728006f-495d-4f19-928a-bb07588869f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 8))\n",
    "sns.scatterplot(\n",
    "    df.groupby(\n",
    "        [\"year\", \"month\", \"day\", \"is_consumption\", \"product_type\"],\n",
    "        observed=True,\n",
    "        as_index=False,\n",
    "    )[\"modified_target\"]\n",
    "    .mean()\n",
    "    .assign(date=lambda d: pd.to_datetime(d[[\"year\", \"month\", \"day\"]])),\n",
    "    x=\"date\",\n",
    "    y=\"modified_target\",\n",
    "    hue=\"product_type\",\n",
    "    s=13,\n",
    ")\n",
    "\n",
    "plt.legend(\n",
    "    title=\"Contract type\",\n",
    "    title_fontsize=12,\n",
    "    bbox_to_anchor=(1, 1),\n",
    "    loc=\"upper left\",\n",
    "    borderaxespad=0,\n",
    "    markerscale=3,\n",
    "    frameon=False,\n",
    "    fontsize=11,\n",
    ")\n",
    "\n",
    "plt.yscale(\"symlog\", linthresh=11)\n",
    "\n",
    "plt.title(\n",
    "    \"Average energy consumption (below 0) or production (above 0) per \"\n",
    "    \"day for each contract type\",\n",
    "    fontsize=13,\n",
    ")\n",
    "plt.xlabel(\"Date\", fontsize=10)\n",
    "plt.ylabel(\"Target, log scale\", fontsize=10, rotation=0, labelpad=30)\n",
    "plt.grid(alpha=0.3, c=\"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b08194-8fc1-4dba-8ab1-66f925b738bd",
   "metadata": {},
   "source": [
    "- Seasonal variations in daily average consumption values are significantly smaller in relative terms (i.e., the percentage change between summer and winter) compared to production. This is likely because energy production by prosumers is severely limited in winter due to low temperatures, shorter daylight hours, and a lower angle of sunlight.\n",
    "- The lowest daily average consumption values are observed for general_service contracts, followed by fixed, combined, and spot contracts. While there are small overlaps in consumption levels on certain dates, these four series are still visually distinguishable.\n",
    "- The range of daily average energy production values follows a similar pattern, but with more overlaps: the lowest values are found in general_service, the highest in spot, while fixed and combined contracts fall somewhere in between.\n",
    "\n",
    "Additionally, it would be useful to analyze production and consumption across different counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9580d10-fd58-4b41-bbe1-65b24d7bbf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(\n",
    "    df.groupby(\n",
    "        [\"year\", \"month\", \"day\", \"is_consumption\", \"county\"],\n",
    "        observed=True,\n",
    "        as_index=False,\n",
    "    )[\"modified_target\"]\n",
    "    .mean()\n",
    "    .assign(date=lambda d: pd.to_datetime(d[[\"year\", \"month\", \"day\"]])),\n",
    "    col=\"county\",\n",
    "    hue=\"is_consumption\",\n",
    "    col_wrap=8,\n",
    "    height=1.8,\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.scatterplot, x=\"date\", y=\"modified_target\", s=4, edgecolor=\"none\"\n",
    ")\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.xaxis.set_major_locator(MonthLocator(interval=6))\n",
    "    ax.xaxis.set_major_formatter(DateFormatter(\"%m.%y\"))\n",
    "    ax.tick_params(axis=\"x\", rotation=30)\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(1000))\n",
    "\n",
    "g.set_axis_labels(\"Date\", \"Target\")\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.add_legend(\n",
    "    title=\"Target type\",\n",
    "    label_order=[\"production\", \"consumption\"],\n",
    "    markerscale=4,\n",
    "    bbox_to_anchor=(0.9, 0.9),\n",
    "    loc=\"upper left\",\n",
    "    borderaxespad=0,\n",
    "    frameon=False,\n",
    "    fontsize=11,\n",
    ")\n",
    "g.legend.get_title().set_fontsize(12)\n",
    "g.figure.suptitle(\n",
    "    \"Average energy consumption (below 0) or production (above 0) per \"\n",
    "    \"week for each county\",\n",
    "    fontsize=13,\n",
    ")\n",
    "g.figure.subplots_adjust(top=0.84, wspace=0.15, hspace=0.35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea44e12-b377-484f-a07d-cca11dd291e5",
   "metadata": {},
   "source": [
    "- Harjumaa and Tartumaa have the highest average energy production and consumption levels, likely due to the presence of large-scale energy consumers and producers in these counties. It is also probable that many consumers in these areas have combined or fixed contract types.\n",
    "- Some counties, such as Hiiumaa, exhibit low and relatively stable energy production and/or consumption levels.\n",
    "- Certain counties display abrupt changes in the graphs that do not repeat. For example, Valgamaa experiences a sharp increase in electricity production toward the end of the observation period, Ida-Virumaa sees the emergence of new consumption levels during the first half of the period, and unknown county undergoes a sharp decline in consumption levels (which later returns to a previous level), whereas in Valgamaa, the decline does not reverse. These changes are most likely due to fluctuations in the number of installed solar panels and batteries.\n",
    "- Seasonal patterns are evident in almost all graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e382f-399a-41bd-8d04-d63015ceccc3",
   "metadata": {},
   "source": [
    "### Time Series Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c21e68-cb35-4d4b-a0fd-c47ed206889d",
   "metadata": {},
   "source": [
    "[Time Series Gaps](#Time-Series-Gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e54828-e52e-4a68-b6e8-c03260f46ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_df(\n",
    "    df: pd.DataFrame, c: str, pt: str, isb: str, isc: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter DataFrame by county, product_type, is_business,\n",
    "    is_consumption. And return DataFrame with datetime index and\n",
    "    target.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df[\n",
    "            (df.county == c)\n",
    "            & (df.product_type == pt)\n",
    "            & (df.is_business == isb)\n",
    "            & (df.is_consumption == isc)\n",
    "        ][[\"datetime\", \"target\"]]\n",
    "        .set_index(\"datetime\")\n",
    "        .copy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61147c67-5636-40f9-a170-05458126b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposition = seasonal_decompose(\n",
    "#     ex_df3,\n",
    "#     model=\"additive\",\n",
    "#     period=24,\n",
    "# )\n",
    "# fig = decomposition.plot()\n",
    "# fig.set_size_inches((4, 3))\n",
    "# fig.tight_layout()\n",
    "# plt.xticks(rotation='vertical')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90b6277-11c9-44c3-8530-5f6b1cc1d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected timeseries\n",
    "decompose_cat = [\n",
    "    [\"harjumaa\", \"fixed\", \"not_business\", \"consumption\"],\n",
    "    [\"ida-virumaa\", \"spot\", \"business\", \"consumption\"],\n",
    "    [\"tartumaa\", \"fixed\", \"business\", \"production\"],\n",
    "    [\"valgamaa\", \"spot\", \"business\", \"production\"],\n",
    "]\n",
    "\n",
    "plot_components = [\"Observed\", \"Trend\", \"Seasonal\", \"Residual\"]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(14, 6), sharex=\"col\")\n",
    "\n",
    "for row, name in enumerate(plot_components):\n",
    "    axes[row, 0].set_ylabel(name)\n",
    "\n",
    "for i, categories in enumerate(decompose_cat):\n",
    "    decomp = seasonal_decompose(\n",
    "        # 24 hour period\n",
    "        example_df(df, *categories), model=\"additive\", period=24\n",
    "    )\n",
    "    axes[0, i].set_title(categories[0], fontsize=11)\n",
    "    axes[0, i].plot(decomp.observed)\n",
    "    axes[1, i].plot(decomp.trend)\n",
    "    axes[2, i].plot(decomp.seasonal)\n",
    "    axes[3, i].plot(decomp.resid)\n",
    "\n",
    "    for ax in axes[:, i]:\n",
    "        ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.875, wspace=0.25, hspace=0.3)\n",
    "fig.suptitle(\n",
    "    \"Seasonal decomposition for different timeseries using moving averages\",\n",
    "    fontsize=13,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b6e5f-15cd-45ea-b9f9-ebdaf4b997f7",
   "metadata": {},
   "source": [
    "The removal of the 24-hour component from the timeseries is not sufficient; both the trend and residual components still exhibit cyclic patterns (including both low-frequency and high-frequency cycles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3573b-1909-4f53-a39b-9e5d319ae95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [24, 24 * 7, 24 * 30]\n",
    "mstl = MSTL(example_df(df, *decompose_cat[0]), periods=periods)\n",
    "res = mstl.fit()\n",
    "\n",
    "fig = res.plot()\n",
    "fig.set_size_inches((16, 12))\n",
    "fig.tight_layout()\n",
    "# plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f21e91-6521-4e6a-b532-87fd6af612c5",
   "metadata": {},
   "source": [
    "Adding only weekly and monthly cycles is insufficient: the trend component still exhibits at least annual seasonality, but with fewer than two full years of data, MSTL cannot reliably estimate yearly effects. The residuals also display persistent cyclic patterns. Moreover, the amplitude of the weekly and monthly seasonal components increases with the trend - suggesting that a multiplicative model or alternative methods would be more appropriate in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b9d45-b9b9-4511-95e9-31c099fd6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = Prophet(\n",
    "#     yearly_seasonality=True,\n",
    "#     weekly_seasonality=True,\n",
    "#     daily_seasonality=True,\n",
    "#     seasonality_mode='multiplicative',\n",
    "# )\n",
    "# m.fit(example_df(df, *decompose_cat[2]).reset_index().rename(columns={\"datetime\": \"ds\", \"target\": \"y\"}))\n",
    "\n",
    "# future = m.make_future_dataframe(periods=24*60, freq='h')\n",
    "# forecast = m.predict(future)\n",
    "\n",
    "# fig1 = m.plot(forecast)\n",
    "\n",
    "\n",
    "# fig2 = m.plot_components(forecast)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d01c5a1-c669-4e61-88ce-cb27db266542",
   "metadata": {},
   "source": [
    "## Client features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180fec6f-8892-479b-85e5-260c9d15dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity_eic = (\n",
    "    df.groupby(\n",
    "        [\"year\", \"month\", \"county\"],\n",
    "        observed=True,\n",
    "        as_index=False,\n",
    "    )\n",
    "    .agg(\n",
    "        installed_capacity=(\"installed_capacity\", \"mean\"),\n",
    "        eic_count=(\"eic_count\", \"mean\"),\n",
    "    )\n",
    "    .assign(date=lambda x: pd.to_datetime(x[[\"year\", \"month\"]].assign(day=15)))\n",
    "    .drop(columns=[\"year\", \"month\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea92b8ff-4f1a-4b7f-8a6a-b79a1ff82e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twin_scatterplot(data, x, y, color, label, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    twin_ax = ax.twinx()\n",
    "    sns.scatterplot(\n",
    "        data=data, x=x, y=y, color=color, ax=twin_ax, s=22, **kwargs\n",
    "    )\n",
    "    twin_ax.tick_params(axis=\"y\", colors=color)\n",
    "    twin_ax.yaxis.label.set_color(color)\n",
    "    facet_idx = ax.get_subplotspec().colspan.start\n",
    "    if (facet_idx + 1) % n_cols == 0:\n",
    "        twin_ax.set_ylabel(label, fontsize=11)\n",
    "    else:\n",
    "        twin_ax.set_ylabel(\"\")\n",
    "    twin_axes.append(twin_ax)\n",
    "\n",
    "\n",
    "twin_axes = []\n",
    "n_cols = 8\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    capacity_eic,\n",
    "    col=\"county\",\n",
    "    col_wrap=n_cols,\n",
    "    sharex=True,\n",
    "    sharey=\"row\",\n",
    "    height=2.5,\n",
    "    aspect=1,\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"date\",\n",
    "    y=\"installed_capacity\",\n",
    "    color=\"tab:blue\",\n",
    "    label=\"Installed_capacity\"\n",
    ")\n",
    "g.map_dataframe(\n",
    "    twin_scatterplot,\n",
    "    x=\"date\",\n",
    "    y=\"eic_count\",\n",
    "    color=\"tab:orange\",\n",
    "    label=\"EIC count\",\n",
    ")\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_ylabel(\"Installed capacity\", fontsize=11, color=\"tab:blue\")\n",
    "    ax.xaxis.set_major_locator(MonthLocator(interval=6))\n",
    "    ax.tick_params(axis=\"x\", rotation=30)\n",
    "    ax.tick_params(axis=\"y\", colors=\"tab:blue\")\n",
    "\n",
    "\n",
    "for i in range(0, len(twin_axes), n_cols):\n",
    "    row_axes = twin_axes[i : i + n_cols]\n",
    "    ylims = [ax.get_ylim() for ax in row_axes if len(ax.collections) > 0]\n",
    "    if ylims:\n",
    "        ymin = min(lim[0] for lim in ylims)\n",
    "        ymax = max(lim[1] for lim in ylims)\n",
    "        for ax in row_axes:\n",
    "            ax.set_ylim(ymin, ymax)\n",
    "\n",
    "for idx, ax in enumerate(g.axes.flat):\n",
    "    if idx % 8 != 0:\n",
    "        ax.tick_params(\n",
    "            axis=\"y\",\n",
    "            which=\"both\",\n",
    "            left=False,\n",
    "            labelleft=False,\n",
    "        )\n",
    "\n",
    "for idx, twin_ax in enumerate(twin_axes):\n",
    "    if (idx + 1) % 8 != 0:\n",
    "        twin_ax.set_yticklabels([])\n",
    "        twin_ax.tick_params(axis=\"y\", which=\"both\", length=0)\n",
    "    else:\n",
    "        label = ax.get_ylabel()\n",
    "        ax.set_ylabel(label, fontsize=11)\n",
    "\n",
    "legend_elements = [\n",
    "    circle_label(\"tab:blue\", \"Installed capacity\"),\n",
    "    circle_label(\"tab:orange\", \"EIC count\"),\n",
    "]\n",
    "\n",
    "g.figure.legend(\n",
    "    title=\"Client feature\",\n",
    "    title_fontsize=12,\n",
    "    handles=legend_elements,\n",
    "    loc=\"upper right\",\n",
    "    frameon=False,\n",
    "    bbox_to_anchor=(1, 0.95),\n",
    "    fontsize=11,\n",
    ")\n",
    "\n",
    "g.figure.suptitle(\n",
    "    \"Average installed capacity and EIC count per month for each county\",\n",
    "    fontsize=13,\n",
    ")\n",
    "sns.despine(fig=g.fig, left=True, bottom=True, right=True, top=True)\n",
    "g.figure.subplots_adjust(top=0.88,right=0.87, wspace=0.05, hspace=0.225)\n",
    "g.set_titles(col_template=\"{col_name}\", size=12)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea6e0d8c-be64-4b5b-94b4-08ebfe3072bd",
   "metadata": {},
   "source": [
    "### Compare different aggregations of weather forecast - historical weather.\n",
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed618a-d92a-4a17-a9ac-5c8b1a68211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hours = df.copy()\n",
    "\n",
    "# hours_ago = (\n",
    "#     [i for i in range(1, 25)]\n",
    "#     + [24 * i for i in range(2, 8)]\n",
    "#     + [168 * i for i in range(2, 9)]\n",
    "#     + [672 * i for i in range(3, 13)]\n",
    "# )\n",
    "# for h in hours_ago:\n",
    "#     df_hours[f\"tm_{h}h\"] = df_hours[\"modified_target\"].shift(h)\n",
    "# df_hours.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a35937-f580-488f-ae59-ef17e4fc5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"tm_1h\"] = df[\"modified_target\"].shift(1)\n",
    "# def add_lags(df):\n",
    "#     target_map = df['PJME_MW'].to_dict()\n",
    "#     df['lag1'] = (df.index - pd.Timedelta('364 days')).map(target_map)\n",
    "#     df['lag2'] = (df.index - pd.Timedelta('728 days')).map(target_map)\n",
    "#     df['lag3'] = (df.index - pd.Timedelta('1092 days')).map(target_map)\n",
    "#     return df\n",
    "# df_label = pd.get_dummies(df_label, drop_first=True)\n",
    "# df_label.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a961c50-b70a-454a-acfb-911ff580a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_label.drop(\n",
    "#     columns=[\n",
    "#         \"target\",\n",
    "#         \"data_block_id\",\n",
    "#         \"row_id\",\n",
    "#         \"prediction_unit_id\",\n",
    "#         \"modified_target\",\n",
    "#     ],\n",
    "#     axis=1,\n",
    "# )\n",
    "\n",
    "# y = df_label[\"modified_target\"].values\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.20, random_state=RAND\n",
    "# )\n",
    "\n",
    "# st = StandardScaler()\n",
    "# X_train_std = st.fit_transform(X_train)\n",
    "# X_test_std = st.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab19d1-f223-4c57-b205-fc5dc087d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def r2_adjusted(\n",
    "#     y_true: np.ndarray, y_pred: np.ndarray, X_test: np.ndarray | int\n",
    "# ) -> float:\n",
    "#     \"\"\"ÐšÐ¾ÑÑ„Ñ„Ð¸Ñ†Ð¸ÐµÐ½Ñ‚ Ð´ÐµÑ‚ÐµÑ€Ð¼Ð¸Ð½Ð°Ñ†Ð¸Ð¸ (Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð°Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ)\"\"\"\n",
    "#     N_objects = len(y_true)\n",
    "\n",
    "#     if isinstance(X_test, np.ndarray):\n",
    "#         N_features = X_test.shape[1]\n",
    "#     else:\n",
    "#         N_features = X_test\n",
    "\n",
    "#     #     N_features = X_test.shape[1]\n",
    "#     r2 = r2_score(y_true, y_pred)\n",
    "#     return 1 - (1 - r2) * (N_objects - 1) / (N_objects - N_features - 1)\n",
    "\n",
    "\n",
    "# def mpe(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "#     \"\"\"Mean percentage error\"\"\"\n",
    "#     return np.mean((y_true - y_pred) / y_true, axis=0) * 100\n",
    "\n",
    "\n",
    "# def mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "#     \"\"\"Mean absolute percentage error\"\"\"\n",
    "#     return np.mean(np.abs((y_pred - y_true) / y_true), axis=0) * 100\n",
    "\n",
    "\n",
    "# def wape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "#     \"\"\"Weighted Absolute Percent Error\"\"\"\n",
    "#     return np.sum(np.abs(y_pred - y_true)) / np.sum(y_true) * 100\n",
    "\n",
    "\n",
    "# def huber_loss(\n",
    "#     y_true: np.ndarray | pd.DataFrame,\n",
    "#     y_pred: np.ndarray | pd.DataFrame,\n",
    "#     delta: float = 1.345,\n",
    "# ):\n",
    "#     \"\"\"Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Ð¥ÑŒÑŽÐ±ÐµÑ€Ð°\"\"\"\n",
    "\n",
    "#     if isinstance(y_true, pd.DataFrame):\n",
    "#         y_true = y_true.squeeze().to_numpy()\n",
    "#     if isinstance(y_pred, pd.DataFrame):\n",
    "#         y_pred = y_pred.squeeze().to_numpy()\n",
    "\n",
    "#     assert len(y_true) == len(y_pred), \"Ð Ð°Ð·Ð½Ñ‹Ðµ Ñ€Ð°Ð·Ð¼ÐµÑ€Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…\"\n",
    "#     huber_sum = 0\n",
    "#     for i in range(len(y_true)):\n",
    "#         if abs(y_true[i] - y_pred[i]) <= delta:\n",
    "#             huber_sum += 0.5 * (y_true[i] - y_pred[i]) ** 2\n",
    "#         else:\n",
    "#             huber_sum += delta * (abs(y_true[i] - y_pred[i]) - 0.5 * delta)\n",
    "#     huber_sum /= len(y_true)\n",
    "#     return huber_sum\n",
    "\n",
    "\n",
    "# def logcosh(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "#     \"\"\"Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Ð›Ð¾Ð³-ÐšÐ¾Ñˆ\"\"\"\n",
    "#     return np.sum(np.log(np.cosh(y_true - y_pred)))\n",
    "\n",
    "\n",
    "# def rmsle(y_true: np.ndarray, y_pred: np.ndarray) -> np.float64:\n",
    "#     \"\"\"\n",
    "#     Root Mean Squared Log Error (RMSLE) metric\n",
    "#     Ð›Ð¾Ð³Ð°Ñ€Ð¸Ñ„Ð¼Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° ÑÑ€ÐµÐ´Ð½ÐµÐ¹ ÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚Ð¸Ñ‡Ð½Ð¾Ð¹ Ð¾ÑˆÐ¸Ð±ÐºÐ¸\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "#     except:\n",
    "#         return None\n",
    "\n",
    "\n",
    "# def get_metrics(\n",
    "#     y_test: np.ndarray,\n",
    "#     y_pred: np.ndarray,\n",
    "#     X_test: np.ndarray,\n",
    "#     name: str = None,\n",
    "#     delta: float = 1.345,\n",
    "# ):\n",
    "#     \"\"\"Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ñ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼Ð¸\"\"\"\n",
    "#     df_metrics = pd.DataFrame()\n",
    "#     df_metrics[\"model\"] = [name]\n",
    "\n",
    "#     df_metrics[\"MAE\"] = mean_absolute_error(y_test, y_pred)\n",
    "#     df_metrics[\"MSE\"] = mean_squared_error(y_test, y_pred)\n",
    "#     df_metrics[\"Huber_loss\"] = huber_loss(y_test, y_pred, delta)\n",
    "#     df_metrics[\"Logcosh\"] = logcosh(y_test, y_pred)\n",
    "#     df_metrics[\"RMSE\"] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "#     df_metrics[\"RMSLE\"] = rmsle(y_test, y_pred)\n",
    "#     df_metrics[\"R2 adjusted\"] = r2_adjusted(y_test, y_pred, X_test)\n",
    "#     df_metrics[\"MPE_%\"] = mpe(y_test, y_pred)\n",
    "#     df_metrics[\"MAPE_%\"] = mape(y_test, y_pred)\n",
    "#     df_metrics[\"WAPE_%\"] = wape(y_test, y_pred)\n",
    "\n",
    "#     return df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6790fd79-1e77-4e6f-bb8d-af83f47a57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sort_index().sort_values(\n",
    "#     [\"county\", \"is_business\", \"product_type\", \"is_consumption\"],\n",
    "#     kind=\"mergesort\",\n",
    "# )\n",
    "# df.sort_index()\n",
    "# tss = TimeSeriesSplit(n_splits=3, test_size=300_000)\n",
    "\n",
    "# a = {}\n",
    "# for i in range(1000):\n",
    "#     x = 0\n",
    "#     for j in range(20):\n",
    "#         x += np.random.choice([-1, 1])\n",
    "#     a[x] = a.get(x, 0) + 1\n",
    "\n",
    "\n",
    "# sns.barplot(x=list(a.keys()), y=list(a.values()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc3653-4435-4e20-bee9-4da67b4ea8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min(data.keys())\n",
    "# max(data.keys())\n",
    "# len(data.keys())\n",
    "# {k: 0 for (k, 0) in range(min(data.keys()), max(data.keys())) if not in data.keys()}\n",
    "# {k: v*2 for (k,v) in dict1.items()}\n",
    "# {key:value for (key,value) in dictonary.items()}\n",
    "# zip()\n",
    "\n",
    "# for train_idx, val_idx in tss.split(df):\n",
    "# print('1 train:', train_idx)\n",
    "# display(df.iloc[train_idx].tail(5))\n",
    "# print('1 val:', val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76417d13-25db-4e88-9af6-04d9e23b6afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tss = TimeSeriesSplit(n_splits=3, test_size=300_000)\n",
    "# df = df.sort_index()\n",
    "\n",
    "# fig, axs = plt.subplots(3, 1, figsize=(10, 10), sharex=True)\n",
    "# fold = 0\n",
    "\n",
    "# for train_idx, val_idx in tss.split(df):\n",
    "#     train = df.iloc[train_idx]\n",
    "#     test = df.iloc[val_idx]\n",
    "#     train[\"modified_target\"].plot(\n",
    "#         ax=axs[fold],\n",
    "#         label=\"Training Set\",\n",
    "#         title=f\"Data Train/Test Split Fold {fold}\",\n",
    "#     )\n",
    "#     test[\"modified_target\"].plot(ax=axs[fold], label=\"Test Set\")\n",
    "#     axs[fold].axvline(test.index.min(), color=\"black\", ls=\"--\")\n",
    "#     fold += 1\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4637896-c557-496a-bad6-0f7734fa9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold = 0\n",
    "# preds = []\n",
    "# scores = []\n",
    "# for train_idx, val_idx in tss.split(df):\n",
    "#     train = df.iloc[train_idx]\n",
    "#     test = df.iloc[val_idx]\n",
    "\n",
    "#     reg = XGBRegressor(\n",
    "#         n_estimators=2000,\n",
    "#         early_stopping_rounds=50,\n",
    "#         objective=\"reg:squarederror\",\n",
    "#         enable_categorical=True,\n",
    "#         eval_metric=\"mae\",\n",
    "#         # max_depth=3,\n",
    "#         learning_rate=0.01,\n",
    "#         random_state=RAND,\n",
    "#     )\n",
    "#     FEATURES = [\n",
    "#         \"county\",\n",
    "#         \"is_business\",\n",
    "#         \"product_type\",\n",
    "#         # 'target',\n",
    "#         \"is_consumption\",\n",
    "#         # 'data_block_id',\n",
    "#         # 'row_id',\n",
    "#         # 'prediction_unit_id',\n",
    "#         \"hour\",\n",
    "#         \"day_of_week\",\n",
    "#         \"day\",\n",
    "#         \"week_of_year\",\n",
    "#         \"month\",\n",
    "#         \"quarter\",\n",
    "#         \"year\",\n",
    "#         # 'modified_target',\n",
    "#     ]\n",
    "#     TARGET = \"modified_target\"\n",
    "\n",
    "#     X_train = train[FEATURES]\n",
    "#     y_train = train[TARGET]\n",
    "\n",
    "#     X_test = test[FEATURES]\n",
    "#     y_test = test[TARGET]\n",
    "\n",
    "#     reg.fit(\n",
    "#         X_train,\n",
    "#         y_train,\n",
    "#         eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "#         verbose=20,\n",
    "#     )\n",
    "\n",
    "#     y_pred = reg.predict(X_test)\n",
    "#     preds.append(y_pred)\n",
    "#     score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "#     scores.append(score)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
