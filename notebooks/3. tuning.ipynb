{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8564d41a-78cf-4436-9f3c-0a83002479b4",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98bc4f-6cb6-4cf4-9d8e-3ddb51c322ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().resolve().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa109d03-2ed5-49bb-a248-5cb63aff28ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame, Series, Timedelta\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44029df-5480-4fc2-bf60-020dec1e0c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loading import load_all_raw_data\n",
    "from utils.preprocessing import process_all_dfs\n",
    "from utils.merging import merge_all_dfs\n",
    "from utils.feature_engineering import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8390c843-629b-4539-a5a2-9ab0e6bd2e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\n",
    "    \"display.float_format\",\n",
    "    lambda x: f\"{x:.2e}\" if abs(x) < 0.01 and x != 0 else f\"{x:.2f}\",\n",
    ")\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaadc0f-2ff7-4e14-86af-d08b7903cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = \"../data/raw_data/\"\n",
    "ADDITIONAL_DATA_PATH = \"../data/additional_data/\"\n",
    "\n",
    "SEGMENT_C = [\"county\", \"product_type\", \"is_business\"]\n",
    "CATEGORICAL_C = [\"county\", \"product_type\", \"is_business\", \"is_consumption\"]\n",
    "TARGET_C = [\n",
    "    \"county\",\n",
    "    \"product_type\",\n",
    "    \"is_business\",\n",
    "    \"is_consumption\",\n",
    "    \"datetime\",\n",
    "]\n",
    "RAND = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b008800-bc93-4537-af46-0be6b76438a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dfs = process_all_dfs(\n",
    "    load_all_raw_data(RAW_DATA_PATH, ADDITIONAL_DATA_PATH)\n",
    ")\n",
    "\n",
    "# processed_dfs.keys()\n",
    "df = merge_all_dfs(processed_dfs, how=\"left\")\n",
    "df = add_dst_flag(df)\n",
    "df = add_cyclic_datetime_features(df, drop_raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58200c-b23e-4e91-a7ea-42838195e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in [2, 3, 7]:\n",
    "# for lag in range(2, 15):\n",
    "    df = df.merge(\n",
    "        get_lag(processed_dfs[\"train\"][TARGET_C + [\"target\"]], lag=lag),\n",
    "        how=\"left\",\n",
    "        on=TARGET_C,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2660bda6-bf83-4fd7-8b0b-4393d93271e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in [24, 24 * 3, 24 * 7, 24 * 14]:\n",
    "    # for window in [24 * i for i in range(1, 15)]:\n",
    "    df = df.merge(\n",
    "        get_moving_average(\n",
    "            processed_dfs[\"train\"]\n",
    "            .set_index(\"datetime\")\n",
    "            .sort_index()\n",
    "            .groupby(CATEGORICAL_C, observed=True, as_index=False),\n",
    "            columns=[\"target\"],\n",
    "            window=window,\n",
    "            # ).dropna(),\n",
    "        ),\n",
    "        how=\"left\",\n",
    "        on=TARGET_C,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc71be1-ec2a-4d15-bbc0-b14a35895020",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"t_over_cap\"] = (df[\"2d_lag_target\"] / df[\"installed_capacity\"]).astype(\n",
    "    \"float32\"\n",
    ")\n",
    "df[\"t_over_eic\"] = (df[\"2d_lag_target\"] / df[\"eic_count\"]).astype(\"float32\")\n",
    "df[\"cap_per_eic\"] = (df[\"installed_capacity\"] / df[\"eic_count\"]).astype(\n",
    "    \"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e96a5-e4b9-4f3c-a5a7-5cf40404e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_TO_DROP = [\"datetime\", \"data_block_id\", \"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e258b-a4a8-49d7-9689-17f650f5d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SPLITS = 2\n",
    "FH = 7 # weekly retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1272eb-bc9b-4568-89b0-1dfff132493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train - val - test\n",
    "# .64 - .16 - .20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66361c00-9f84-43f6-9aa1-9c67be301593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation period starts: 2022-10-14 00:00:00\n",
      "Test period starts: 2023-01-24 00:00:00\n"
     ]
    }
   ],
   "source": [
    "val_dt_start = (\n",
    "    df[\"datetime\"].min() + (df[\"datetime\"].max() - df[\"datetime\"].min()) * 0.64\n",
    ").normalize()\n",
    "test_dt_start = (\n",
    "    df[\"datetime\"].min() + (df[\"datetime\"].max() - df[\"datetime\"].min()) * 0.8\n",
    ").normalize()\n",
    "print(\n",
    "    f\"Validation period starts: {val_dt_start}\",\n",
    "    f\"Test period starts: {test_dt_start}\",\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4451880f-2d0b-40f0-a122-e86cc3240433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_days_range = (\n",
    "    val_dt_start - Timedelta(hours=1) - df[\"datetime\"].min()\n",
    ").days\n",
    "train_days_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d8c0fd-83a4-42ce-a64a-6d820f0c2b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_fixed = split_by_equal_days(\n",
    "    dt=df.loc[df[\"datetime\"] < test_dt_start, \"datetime\"],\n",
    "    train_days=train_days_range,\n",
    "    fh=FH, \n",
    "    n_splits=VAL_SPLITS,\n",
    "    expanding=False,\n",
    ")\n",
    "\n",
    "# new_splits = []\n",
    "\n",
    "# for i, d in enumerate(splits_fixed):\n",
    "#     count = len(\n",
    "#         df.loc[\n",
    "#             (df[\"datetime\"] >= d[\"val\"][0]) & (df[\"datetime\"] <= d[\"val\"][1])\n",
    "#         ]\n",
    "#     )\n",
    "#     if count < 24:\n",
    "#         print(f\"Split {i} only has {count} rows, removed\")\n",
    "#     else:\n",
    "#         new_splits.append(d)\n",
    "\n",
    "# splits_fixed = new_splits\n",
    "# for i, d in enumerate(splits_fixed):\n",
    "#     print(i, \"train\", d[\"train\"])\n",
    "#     print(i, \"valid\", d[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6dcebe-0228-4653-86fb-d8dbf8e70d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_mae_history = []\n",
    "\n",
    "# xgb_p = {\n",
    "#     \"n_estimators\": 100,\n",
    "#     \"learning_rate\": 0.1,\n",
    "#     \"max_depth\": 7,\n",
    "#     \"random_state\": RAND,\n",
    "#     \"subsample\": 0.8,\n",
    "#     \"colsample_bytree\": 0.8,\n",
    "#     \"objective\": \"reg:absoluteerror\",\n",
    "#     \"enable_categorical\": True,\n",
    "#     \"early_stopping_rounds\": 20,\n",
    "#     \"eval_metric\": \"mae\",\n",
    "#     \"n_jobs\": -1,\n",
    "# }\n",
    "# xgbr = XGBRegressor(**xgb_p)\n",
    "\n",
    "# for split in splits_fixed:\n",
    "#     df_train = df[\n",
    "#         (df[\"datetime\"] >= split[\"train\"][0])\n",
    "#         & (df[\"datetime\"] <= split[\"train\"][1])\n",
    "#     ].drop(FEATURES_TO_DROP, axis=1)\n",
    "#     df_val = df[\n",
    "#         (df[\"datetime\"] >= split[\"val\"][0])\n",
    "#         & (df[\"datetime\"] <= split[\"val\"][1])\n",
    "#     ].drop(FEATURES_TO_DROP, axis=1)\n",
    "\n",
    "#     X_train, y_train = (\n",
    "#         df_train.drop([\"target\"], axis=1),\n",
    "#         df_train[\"target\"],\n",
    "#     )\n",
    "#     X_val, y_val = df_val.drop([\"target\"], axis=1), df_val[\"target\"]\n",
    "#     # X_test, y_test = df_test.drop([\"target\"], axis=1), df_test[\"target\"]\n",
    "\n",
    "#     eval_set = [\n",
    "#         (X_train, y_train),\n",
    "#         (X_val, y_val),\n",
    "#         # (X_test, y_test)\n",
    "#     ]\n",
    "#     # print(f\"{i+1} split\")\n",
    "\n",
    "#     # Naive baseline\n",
    "#     naive_mae.append(\n",
    "#         {\n",
    "#             \"validation_0\": MAE(\n",
    "#                 eval_set[0][1].loc[X_train[\"2d_lag_target\"].notna()],\n",
    "#                 eval_set[0][0][\"2d_lag_target\"].loc[\n",
    "#                     X_train[\"2d_lag_target\"].notna()\n",
    "#                 ],\n",
    "#             ),\n",
    "#             \"validation_1\": MAE(\n",
    "#                 eval_set[1][1].loc[X_val[\"2d_lag_target\"].notna()],\n",
    "#                 eval_set[1][0][\"2d_lag_target\"].loc[\n",
    "#                     X_val[\"2d_lag_target\"].notna()\n",
    "#                 ],\n",
    "#             ),\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     XGBRegressor.fit(\n",
    "#         X_train,\n",
    "#         y_train,\n",
    "#         eval_set=eval_set,\n",
    "#         verbose=0,\n",
    "#         # verbose=25,\n",
    "#     )\n",
    "#     xgb_mae_history.append(model.evals_result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edcc600-ed39-415f-b707-adc1f63952af",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTUNA_ESTIMATORS = 1000\n",
    "OPTUNA_ESR = 50\n",
    "VERBOSE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d04a3-6ca8-4bdf-b941-211608d4c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 2000, step=500),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "\n",
    "        \"early_stopping_rounds\": OPTUNA_ESR,\n",
    "        \"random_state\": RAND,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"objective\": \"reg:absoluteerror\",\n",
    "        \"eval_metric\": \"mae\",\n",
    "        \"enable_categorical\": True,\n",
    "        \"n_jobs\": -1,\n",
    "\n",
    "    }\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    splits_fixed = split_by_equal_days(\n",
    "        dt=df.loc[df[\"datetime\"] < test_dt_start, \"datetime\"],\n",
    "        train_days=train_days_range,\n",
    "        fh=7, # weekly retraining\n",
    "        n_splits=VAL_SPLITS,\n",
    "        expanding=False,\n",
    "    )\n",
    "    cv_predicts = np.empty(VAL_SPLITS)\n",
    "\n",
    "    for idx, split in enumerate(splits_fixed):\n",
    "        df_train = df[\n",
    "            (df[\"datetime\"] >= split[\"train\"][0])\n",
    "            & (df[\"datetime\"] <= split[\"train\"][1])\n",
    "        ].drop(FEATURES_TO_DROP, axis=1)\n",
    "        df_val = df[\n",
    "            (df[\"datetime\"] >= split[\"val\"][0])\n",
    "            & (df[\"datetime\"] <= split[\"val\"][1])\n",
    "        ].drop(FEATURES_TO_DROP, axis=1)\n",
    "    \n",
    "        X_train, y_train = (\n",
    "            df_train.drop([\"target\"], axis=1),\n",
    "            df_train[\"target\"],\n",
    "        )\n",
    "        X_val, y_val = df_val.drop([\"target\"], axis=1), df_val[\"target\"]\n",
    "        # X_test, y_test = df_test.drop([\"target\"], axis=1), df_test[\"target\"]\n",
    "        eval_set = [\n",
    "            (X_train, y_train),\n",
    "            (X_val, y_val),\n",
    "            # (X_test, y_test)\n",
    "        ]\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            verbose=VERBOSE,\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        cv_predicts[idx] = MAE(y_val, preds)\n",
    "\n",
    "    return np.mean(cv_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b84977f-eddf-4a35-bad6-d4b690701732",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    pruner=optuna.pruners.SuccessiveHalvingPruner(),\n",
    "    study_name=\"xgb_optuna\",\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=5, show_progress_bar=True, n_jobs=1)\n",
    "# >1h 10 n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096d5d51-62db-4079-a31c-3213abbe5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27f927-d8cb-418f-a95a-09ac2f4209fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for split in [[xgb_mae_history[i][\"validation_1\"][\"mae\"][-1] for i in range(split * 5, split * 5 + 5)] for split in range(4)]:\n",
    "#     print(np.round(np.mean(split), 3))\n",
    "\n",
    "# for split in [[lgbm_mae_history[i][\"valid_1\"][\"l1\"][-1] for i in range(split * 5, split * 5 + 5)] for split in range(4)]:\n",
    "#     print(np.round(np.mean(split), 3))\n",
    "\n",
    "# for split in [[cb_mae_history[i][\"validation_1\"][\"MAE\"][-1] for i in range(split * 5, split * 5 + 5)] for split in range(4)]:\n",
    "#     print(np.round(np.mean(split), 3))\n",
    "\n",
    "# [[i for i in range(split * 5, split * 5 + 5)] for split in range(4)]\n",
    "\n",
    "# residuals = y_test - y_pred\n",
    "\n",
    "# plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "# plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Residuals\")\n",
    "# plt.title(\"Residuals vs Predicted\")\n",
    "# plt.show()\n",
    "\n",
    "# fi = pd.Series(xgbr.feature_importances_, index=X_train.columns)\n",
    "# fi = fi.sort_values(ascending=False).head(20)\n",
    "\n",
    "# fi.plot.barh(figsize=(8,6))\n",
    "# plt.xlabel(\"Feature Importance\")\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.title(\"Top 20 Important Features\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
